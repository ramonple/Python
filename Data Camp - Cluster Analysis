
################################################  Course 1: Introduction to Clustering ########################
# Hierarchical clustering in SciPy
from scipy.cluster.hierarchy import linkage, fcluster
x_coordinates =[90,80,38,....]
y_coordinates = [1,2,3,4,.....]
df=pd.DataFrame({'x_coordinate':x_coordinates,
                'y_coordinate': y_coordinates})
                
# Use the linkage() function to compute distances using the ward method               
Z = linkage(df,'ward') -> the linkage method computes distances between intermediate clusters

# Generate cluster labels for each data point with two clusters using the fcluster() function
df['cluster_labels'] = fcluster(Z,3,criterion='maxcluster') -> the fcluster method generates clusters and assigns associated cluster labels to a new column in the DataFrame

sns.scatterplot(x='',y='',hue='',data=)

# K-Means 
from scipy.cluster.vq import kmeans, vq

random.seed((100,200))
# The centroids of the clusters are computed using kmeans and cluster assignments for each point are done through vq.
centroids,- = kmeans(df,3)
df['cluster_labels'],_ = vq(df,centroids)

---- exercise
# Import linkage and fcluster functions
from scipy.cluster.hierarchy import linkage,fcluster

# Use the linkage() function to compute distance
Z = linkage(df, 'ward')

# Generate cluster labels
df['cluster_labels'] = fcluster(Z, 2, criterion='maxclust')

# Plot the points with seaborn
sns.scatterplot(x='x', y='y', hue='cluster_labels', data=df) --> hue !!!
plt.show()





--- Data preparation for cluster analysis
Why?
1. incomparable units
2. variables with some units have vastly different scales and variances
3. data in raw form may lead to bias
solution: Normalisation -> rescaling data to a std = 1
x_new = x/std_dev(x)

from scipy.cluster.vq import whiten
data=[1,2,3,4,5,6]
scaled_data = whiten(data)

################################################  Course 2: Hierarchical Clustering ################################################ 

# creating a distance matrix using linkage
scipy.cluster.hierarchy.linkage(observations,method='single',metric='euclidean',optimal_ordering=False)
# method: single,complete,average,centroid,median,ward

# create cluster labels
scipy.cluster.hierarchy.fcluster(distance_matrix,num_clusters,criterion) --> num_cluster be the num directly
 --  distance_matrix: output of linkage() method
 
 -----> exercise
 # Import the fcluster and linkage functions
from scipy.cluster.hierarchy import linkage,fcluster

# Use the linkage() function
distance_matrix = linkage(comic_con[['x_scaled', 'y_scaled']], method = 'ward', metric = 'euclidean')

# Assign cluster labels
comic_con['cluster_labels'] =fcluster(distance_matrix, 2, criterion='maxclust')

# Plot clusters
sns.scatterplot(x='x_scaled', y='y_scaled', 
                hue='cluster_labels', data = comic_con)
plt.show()




----- Visualize clusters
df =pd.DataFrame({'x':[],'y':[],'labels':['a','b','c']})
colors ={'A':'red','B':'blue'}
df.plot.scatter(x='x',y='y',

sns.scatter(x='x',y='x',hue='labels',data=df)


----- exercise
# Import the pyplot class
from matplotlib import pyplot as plt

# Define a colors dictionary for clusters
colors = {1:'red', 2:'blue'}

# Plot a scatter plot
comic_con.plot.scatter(x='x_scaled', 
                	   y='y_scaled',
                       c=comic_con['cluster_labels'].apply(lambda x: colors[x])) --> check how to set the colors
plt.show()






--- How many clusters?
# create a dendrogram in Scipy
from scipy.cluster.hierarchy import dendrogram
z= linkage(df[['x_whiten'],['y_whiten']], method ='ward',metric='euclidean')
dn = dendrogram(Z)



-- Limitations of hierarchical clustering
# Measuring speed in hierarchical clustering
timeit module
measure the speed of .linkage() method
use randomly generated points

from scipy.cluster.hierarchy
import timeit,random
points = 100
df=pd.DataFrame({'x':random.sample(range(0,points),points),
                 'y':random.sample(range(0,points),points)})
% timeit linkage(df['x','y']],method='ward',metric='euclidean')
--> output: 1.02ms +- 133 mius per loop (mean +- std.dev of 7 runs, 1000 loops each)

--- exercise
# Fit the data into a hierarchical clustering algorithm
distance_matrix = linkage(fifa[['scaled_sliding_tackle', 'scaled_aggression']], 'ward')

# Assign cluster labels to each row of data
fifa['cluster_labels'] = fcluster(distance_matrix, 3, criterion='maxclust')

# Display cluster centers of each cluster
print(fifa[['scaled_sliding_tackle', 'scaled_aggression', 'cluster_labels']].groupby('cluster_labels').mean())

# Create a scatter plot through seaborn
sns.scatterplot(x='scaled_sliding_tackle', y='scaled_aggression', hue='cluster_labels',data=fifa)
plt.show()



################################################  Course 3: K-Means Clustering ################################################ 
Step 1: generate cluster centers
kmeans(obs,k_or_guess,iter,thresh,check_finite)
 - obs: strandardized observarions
 - k_or_guess: number of clusters
 - iter: number of iterations (default =20 )
 - thres: threshold (default = 1e-05)
 - check_finite: True/ False whether to check if observarions contain only finite numbers (default = True)
 
 -> return: cluster centers, distortion
 distortion = sum of squares of distances of points from cluster centers
 
Step 2: generate cluster labels
vq(obs,code_book, check_finite=True)
- obs: standardised observations
- code_book: cluster centers, the output from step 1
- check_finite: True/ False whether to check if observarions contain only finite numbers (default = True)

-> Return: a list of cluster labels, a list of distortions


# A note on distortions
kmeans: return a single value of distortions
vq: return a list of distortions


from scipy.cluster.vq import kmeans, vq
cluster_centers,_ = kmeans(df[['scaled_x','scaled_y']],3)
df['cluster_labels'],_ = vq(df[['scaled_x','scaled_y']],cluster_centers)




---- How many clusters?
Elbow method
Elbow plot: plot of number of clusters and disotrtion

distortions=[]
num_clusters = range(2,7)

for i in num_clusters:
    centroids,distortion = kmeans(df[['x_scaled','y_scaled']],i)
    distortions.append(distortion)
    
elbow_plot_data = pd.DataFrame({'num_clusters':num_clusters, 'distortion':distortions})
sns.lineplot(x='num_clusters',y='distortion',data=elbow_plot_data)



---- Limitations of k-means clustering
how to find the right K?
impact of seeds
biased towards equal sized clusters

from numpy import random
random.seed(12)
random.seed([1, 2, 1000])



# Import the kmeans and vq functions
from scipy.cluster.vq import kmeans, vq

-- exercise

# Set up a random seed in numpy
random.seed([1000,2000])

# Fit the data into a k-means algorithm
cluster_centers,_ = kmeans(fifa[['scaled_def', 'scaled_phy']], 3)

# Assign cluster labels
fifa['cluster_labels'], _ = vq(fifa[['scaled_def', 'scaled_phy']], cluster_centers)

# Display cluster centers 
print(fifa[['scaled_def', 'scaled_phy', 'cluster_labels']].groupby('cluster_labels').mean())

# Create a scatter plot through seaborn
sns.scatterplot(x='scaled_def', y='scaled_phy', hue='cluster_labels', data=fifa)
plt.show()


################################################  Course 4: Clustering in Real World ################################################ 
 ---- Dominant colors in images
 convert image to pixels: matplotlib.image.imread
 display colors of cluster centers: matplotlib.pyplot.imshow
 
 # convert image to RGB matrix
 import matplotlib.image as img
 image = img.imread('sea.jpg')
 image.shape
 
 r=[]
 g=[]
 b=[]
 for row in image:
     for pixel in row:
        # A pixel contains RGB values
        temp_r,temp_g,temp_b = pixel
        r.append(temp_r)
        g.append(temp_g)
        b.append(temp_b)

pixels = pd.DataFrame({'red':r, 'blue':b,'green':g})


# create an elbow plot
distortions=[]
num_clusters = range(1,11)

# create a list of distortions from the kmeans method
for i in num_clusters:
     cluster_centers,_ = kmeans(pixles[['scaled_red','scaled_blue','scaled_green']],i)
     distortions.append(distortion)
     
# create a dataframe with two lists - number of clusters and corresponding distortions
elbow_plot = pd.DataFrame({'number_clusters':num_clusters,'distortions':distortion})

# create plot
sns.lineplot(x='num_clusters',y='distortions',data=elbow_plot)
plt.xticks(num_clusters)

# find dominant colors
r_std,g_std,b_std = pixels[['red','blue','green']].std()

# Scaled actual RGB values in range of 0-1
for cluster_center in cluster_centers:
    scaled_r,scaled_g,scaled_b = cluster_center
    colors.apped((
                 scaled_r * r_std/255,
                 scaled_g * g_std/255,
                 scaled_b * b_std/255))

print(colors)
plt.imshow([colors])



---- Document clustering
# convert text into smaller parts called tokens, clean data for processing

from nltk.tokenize import word_tokenize
import re
def remove_noise(text,stop_words =[]):
    tokens = word_tokenize(text)
    cleaned_tokens =[]
    for token in tokens:
        token = re.sub('[^A-Za-z0-9]+','',token)
        if len(token) > 1 and token.lower() not in stop_words:
        # get lowercase
        cleaned_toekns.append(token.lower())
 return cleaned_tokens
remove_nose("it is lovely weather wer are having. I hope the weather continues.")


TF-IDF (term-frequency. - inverse document frequency)
- a weighted measure: evaluate how important a word is to a document in a collection

from sklearn.feature_extraction.text import IfidVectorizer
tfidf_vectorizer = RfidfVectorizer(max_df=0.8,max_feature=50,min_df=0.2,tokenizer=remove_noise)
tfidf_matrix = rfidf_vectorizer.fit_transform(data)

# clustering with sparse matrix
kmeans in Scipy does not support sparse matrices
Use .todense() to convert to a matrix
=> cluster_centers,distortion = kmeans(tfidf_matrix.todense(),num_clusters)




num_clusters = 2

# Generate cluster centers through the kmeans function
cluster_centers, distortion = kmeans(tfidf_matrix.todense(), num_clusters)

# Generate terms from the tfidf_vectorizer object
terms = tfidf_vectorizer.get_feature_names()

for i in range(num_clusters):
    # Sort the terms and print top 3 terms
    center_terms = dict(zip(terms, list(cluster_centers[i])))
    sorted_terms = sorted(center_terms, key=center_terms.get, reverse=True)
    print(sorted_terms[:3])




---- Clustering with multiple features

# Create centroids with kmeans for 2 clusters
cluster_centers,_ = kmeans(fifa[scaled_features], 2)

# Assign cluster labels and print cluster centers
fifa['cluster_labels'], _ = vq(fifa[scaled_features], cluster_centers)
print(fifa.groupby('cluster_labels')[scaled_features].mean())

# Plot cluster centers to visualize clusters
fifa.groupby('cluster_labels')[scaled_features].mean().plot(legend=True, kind='bar')
plt.show()

# Get the name column of first 5 players in each cluster
for cluster in fifa['cluster_labels'].unique():
    print(cluster, fifa[fifa['cluster_labels'] == cluster]['name'].values[:5])





