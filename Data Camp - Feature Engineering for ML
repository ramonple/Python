########################################## Course 1: Creating Features ########################################## 

# select specific data types
df.select_dtypes(include=['int'])



# Import pandas
import pandas as pd

# Import so_survey_csv into so_survey_df
so_survey_df = pd.read_csv(so_survey_csv)

# Print the first five rows of the DataFrame
print(so_survey_df.head())

# Print the data type of each column
print(so_survey_df.dtypes)


# Create subset of only the numeric columns
so_numeric_df = so_survey_df.select_dtypes(include=['int','float'])

# Print the column names contained in so_survey_df_num
print(so_numeric_df.columns)




----- Dealing with categorical features

one hot vs dummies
one hot encoding: explainable features
dummy encoding: necessary information without duplication


Limiting your columns : avoding too many columns for dummy variables

counts=df['xx'].value_counts()
mask = df['xx'].isin(counts[counts < 5].index)
df['xx'][mask] = 'other'


# Convert the Country column to a one hot encoded Data Frame
one_hot_encoded = pd.get_dummies(so_survey_df, columns=['Country'], prefix='OH')

# Print the columns names
print(one_hot_encoded.columns)


# Create dummy variables for the Country column
dummy = pd.get_dummies(so_survey_df, columns=['Country'], drop_first=True, prefix='DM')

# Print the columns names
print(dummy.columns)




# Create a series out of the Country column
countries = so_survey_df['Country']

# Get the counts of each category
country_counts = countries.value_counts()

# Create a mask for only categories that occur less than 10 times
mask = countries.isin(country_counts[country_counts < 10].index)

# Label all other categories as Other
countries[mask] = 'Other'

# Print the updated category counts
print(pd.value_counts(countries))




Numeric variables

Binarizing columns
While numeric values can often be used without any feature engineering, there will be cases when some form of manipulation can be useful. 

# Create the Paid_Job column filled with zeros
so_survey_df['Paid_Job'] = 0

# Replace all the Paid_Job values where ConvertedSalary is > 0
so_survey_df.loc[so_survey_df['ConvertedSalary'] > 0, 'Paid_Job'] = 1

# Print the first five rows of the columns
print(so_survey_df[['Paid_Job', 'ConvertedSalary']].head())


# Bin the value of the ConvertedSalary column in so_survey_df into 5 equal bins, in a new column called equal_binned.
# Bin the continuous variable ConvertedSalary into 5 bins
so_survey_df['equal_binned'] = pd.cut(so_survey_df['ConvertedSalary'], bins=5)

# Print the first 5 rows of the equal_binned column
print(so_survey_df[['equal_binned', 'ConvertedSalary']].head())

# Import numpy
import numpy as np

# Specify the boundaries of the bins
bins = [-np.inf, 10000, 50000, 100000, 150000, np.inf]

# Bin labels
labels = ['Very low', 'Low', 'Medium', 'High', 'Very high']

# Bin the continuous variable ConvertedSalary using these boundaries
so_survey_df['boundary_binned'] = pd.cut(so_survey_df['ConvertedSalary'], 
                                         bins, labels = labels)

# Print the first 5 rows of the boundary_binned column
print(so_survey_df[['boundary_binned', 'ConvertedSalary']].head())



##########################################  Course 2:  Dealing with Messy Data ########################################## 

missing value discovery: df.info()

finding missing value: df.isnull()

missing value numbers:  df['xx'].isnull().sum()

non-missing value: df.notnull()



# Print the locations of the non-missing values
print(sub_df.head(10).notnull())



--- Dealing with missing values:
df.dropna(how='any') # all rows with missing value
df.dropna(subset=['xxx'])

df['xx'].fillna(value='xx',inplace=True)

# record where the values are not miissing
df['xx'] = df['yy'].notnull()


# Create a new DataFrame dropping all columns with incomplete rows
no_missing_values_cols = so_survey_df.dropna(how='any', axis=1)

Use df.dropna(how='any'), with an additional argument, axis set to 1 to drop columns with missing values.


# Replace missing values
so_survey_df['Gender'].fillna(value='Not Given', inplace=True)

# Print the count of each value
print(so_survey_df['Gender'].value_counts())




df['xx'] = df['xx'].fillna(df['xxx'].mean())
df['xx']=df['xx'].astype('int64')





-- other data issues

df['xxx'].dtype

df['xx']=df['xx'].str.replace(',','')

df['xx']=df['xx'].astype()

-- chaning methods
df['xx'] = df['xx'].method1()
df['xx'] = df['xx'].method2()
df['xx'] = df['xx'].method3()

df['xx'] = df['xx']\
           .method1().method2().method3()
           
           
# Attempt to convert the column to numeric values
numeric_vals = pd.to_numeric(so_survey_df['RawSalary'], errors='coerce')

# Find the indexes of missing values
idx = numeric_vals.isna()

# Print the relevant rows
print(so_survey_df['RawSalary'][idx])


# Use method chaining
so_survey_df['RawSalary'] = so_survey_df['RawSalary']\
                              .str.replace(',', '')\
                              .str.replace('$', '')\
                              .str.replace('£', '')\
                              .astype('float')
 
# Print the RawSalary column
print(so_survey_df['RawSalary'])



##########################################  Course 3: Conforming to Statistical Assumptions ########################################## 

-- Data Distributions

import matplot as plt
df.hist()
plt.show()


-- Paring distributions
how different features in your DataFrame interact with each other.
it is useful to see if multiple columns are correlated with each other or whether they have any association at all

import seaborn as sns
sns.pairplot(df)




-- Scaling and transformations


min-max scaling [0,1]

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
scaler.fit(df[['xxx']])
df['xx'] = scaler.transform(df[['xx']])



Standardisation

from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
scaler.fit(df[['xx']])
df['xx'] = scaler\
           .transform(df[['xxx']])
           
           



Log Transformation

from sklearn.preprocessing import PowerTransformer
log = PowerTransformer()
log.fit (df[['xxx']])
df['xx'] = log.transform(df[['xxx']])




---- Removing outliers

- Quantiles in Python
q_cutoff = df['xx'].quantile(0.95)
mask = df['xxx'] < q_cutoff
trimmed_df = df[mask]



- standard deviation detection in Python
mean = df['xxx'].mean()
std=df['xx'].std()

cut_off = std*3

lower,upper = mean-cut_off,mean+cut_off

new_df = df[ (df['xxx'] < upper)  & (df['xxx'] >lower) ]




-- Scaling and transforming new data


# Import StandardScaler
from sklearn.preprocessing import StandardScaler

# Apply a standard scaler to the data
SS_scaler = StandardScaler()

# Fit the standard scaler to the data
SS_scaler.fit(so_train_numeric[['Age']])

# Transform the test data using the fitted scaler
so_test_numeric['Age_ss'] = SS_scaler.transform(so_test_numeric[['Age']])
print(so_test_numeric[['Age', 'Age_ss']].head())



train_std = so_train_numeric['ConvertedSalary'].std()
train_mean = so_train_numeric['ConvertedSalary'].mean()

cut_off = train_std * 3
train_lower, train_upper = train_mean - cut_off, train_mean + cut_off

# Trim the test DataFrame
trimmed_df = so_test_numeric[(so_test_numeric['ConvertedSalary'] < train_upper) \
                             & (so_test_numeric['ConvertedSalary'] > train_lower)]





########################################## Course 4: Dealing with Text Data ########################################## 
prefix: A prefix is the beginning letter of a word or group of words. Example: The word “unhappy” consists of the prefix “un." 


[a-zA-Z]: all letter characters
[^a-zA-Z]: all non letter characters

df['xx'] = df['xx']\
           .str.replace('[^a-zA-Z]','')
           
df['xx'] = df['xx'].str.lower()

df['xx'] = df['xx'].str.len()

df['xx'] = df['xx'].str.split()

df['xx'] = df['xx'].str.split().str.len()


-- word counts

# Import CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer

# Instantiate CountVectorizer
cv = CountVectorizer()

# Fit the vectorizer
cv.fit(speech_df['text_clean'])

# Print feature names
print(cv.get_feature_names())





# Apply the vectorizer
cv_transformed = cv.transform(speech_df['text_clean'])

# Print the full array
cv_array = cv_transformed.toarray()

# Print the shape of cv_array
print(cv_array.shape)



# Import CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer

# Specify arguements to limit the number of features generated
cv = CountVectorizer(min_df=0.2, max_df=0.8)

# Fit, transform, and convert into array
cv_transformed = cv.fit_transform(speech_df['text_clean'])
cv_array = cv_transformed.toarray()

# Print the array shape
print(cv_array.shape)


# Create a DataFrame with these features
cv_df = pd.DataFrame(cv_array, 
                     columns=cv.get_feature_names()).add_prefix('Counts_')

# Add the new columns to the original DataFrame
speech_df_new = pd.concat([speech_df, cv_df], axis=1, sort=False)
print(speech_df_new.head())




---- Term frequency-inverse document frequency

TF-IDF: ( count of word occurances / total words in document) / ( log (number of docs word is in / total number of docs) )



# Import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

# Instantiate TfidfVectorizer
tv = TfidfVectorizer(max_features=100,stop_words='english')

# Fit the vectroizer and transform the data
tv_transformed = tv.fit_transform(speech_df['text_clean'])

# Create a DataFrame with these features
tv_df = pd.DataFrame(tv_transformed.toarray(), 
                     columns=tv.get_feature_names()).add_prefix('TFIDF_')
print(tv_df.head())




# Isolate the row to be examined
sample_row = tv_df.iloc[0]

# Print the top 5 words of the sorted output
print(sample_row.sort_values(ascending=False).head())







# Instantiate TfidfVectorizer
tv = TfidfVectorizer(max_features=100, stop_words='english')

# Fit the vectroizer and transform the data
tv_transformed = tv.fit_transform(train_speech_df['text_clean'])

# Transform test data
test_tv_transformed = tv.transform(test_speech_df['text_clean'])

# Create new features for the test set
test_tv_df = pd.DataFrame(test_tv_transformed.toarray(), 
                          columns=tv.get_feature_names()).add_prefix('TFIDF_')
print(test_tv_df.head())



-- N-grams

# Import CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer

# Instantiate a trigram vectorizer
cv_trigram_vec = CountVectorizer(max_features=100, 
                                 stop_words='english', 
                                 ngram_range = (3,3))

# Fit and apply trigram vectorizer
cv_trigram = cv_trigram_vec.fit_transform(speech_df['text_clean'])

# Print the trigram features
print(cv_trigram_vec.get_feature_names())


# Import CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer

# Instantiate a trigram vectorizer
cv_trigram_vec = CountVectorizer(max_features=100, 
                                 stop_words='english', 
                                 ngram_range = (3,3))

# Fit and apply trigram vectorizer
cv_trigram = cv_trigram_vec.fit_transform(speech_df['text_clean'])

# Print the trigram features
print(cv_trigram_vec.get_feature_names())

