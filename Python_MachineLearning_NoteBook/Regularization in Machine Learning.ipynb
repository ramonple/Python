{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f0b69f",
   "metadata": {},
   "source": [
    "### What is data fitting?\n",
    "The line used to represent the relationship is a straight line that passes through the data \\\n",
    "points and the variables have linear relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b2126",
   "metadata": {},
   "source": [
    "### Bias and Variance\n",
    "Bias occurs when an algorithm has limited flexibility to learn from data: pay very little attention on training data.\n",
    "\n",
    "Variance defines the algorithm's sensitivity to specific sets of data: pay too much attention on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe51f2a",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "A scenario where the machine learning model tries to learn from the details along with the noise in the data and tries to fit each data point on the curve.\n",
    "\n",
    "###### Reason:\n",
    "a. data used for training is not cleaned and contains noise\n",
    "\n",
    "b. The model has high variance\n",
    "\n",
    "c. Size of training data is not enough\n",
    "\n",
    "d. The model is too complex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416bae26",
   "metadata": {},
   "source": [
    "### Underfitting\n",
    "A scenario where machine learning model can neither learn the relationship between variables in the data nor predict or classify a new dataset.\n",
    "\n",
    "##### Reason:\n",
    "\n",
    "a. data used for training is not cleaned and contains noise\n",
    "\n",
    "b. The model has high bias\n",
    "\n",
    "c. Size of training data is not enough\n",
    "\n",
    "d. The model is too simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4add10",
   "metadata": {},
   "source": [
    "## What is Regularization\n",
    "\n",
    "Regularization techiques are used to calibrate linear regression models in order to minimise the adjusted loss function and prevent both overfitting and underfitting.\n",
    "\n",
    "Ridge (L2) Regularization\n",
    "\n",
    "Lasso (L1) Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fe20e5",
   "metadata": {},
   "source": [
    "## Ridge (L2) Regularization\n",
    "\n",
    "It modifies the overfitted or underfitted models by adding the penalty equivalent to the sum of the squares of the magnitude of coefficients.\n",
    "\n",
    "### $ Cost \\, Function = Loss + \\lambda \\times \\sum(w)^2$\n",
    "\n",
    "$ Loss$ = Sum of squared residuals\n",
    "\n",
    "$\\lambda$ = penalty for the errors\n",
    "\n",
    "$w$ = slope of the curve/ line\n",
    "\n",
    "### When to use:\n",
    "Useful when we have many variables with relatively smaller data samples.\n",
    "\n",
    "The model does not encourage convergence towards zero but is likely to make them closer to zero and prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf742b1",
   "metadata": {},
   "source": [
    "## Lasso (R1) Regularization\n",
    "\n",
    "It modifies the overfitted or underfitted models by adding the penalty equivalent to the sum of the absolute values of coefficients.\n",
    "\n",
    "### $ Cost \\, Function = Loss + \\lambda \\times \\sum||w||$\n",
    "\n",
    "$ Loss$ = Sum of squared residuals\n",
    "\n",
    "$\\lambda$ = penalty for the errors\n",
    "\n",
    "$w$ = slope of the curve/ line\n",
    "\n",
    "### When to use:\n",
    "Preferred when we are fitting a linear model with fewer variables.\n",
    "\n",
    "It encourages the coefficients of the variables to go towards zero because of the shape of the constraints of the abosulte value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ca05ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3120e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
      "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
      "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
      "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
      "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
      "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
      "\n",
      "   PTRATIO       B  LSTAT  House Price  \n",
      "0     15.3  396.90   4.98         24.0  \n",
      "1     17.8  396.90   9.14         21.6  \n",
      "2     17.8  392.83   4.03         34.7  \n",
      "3     18.7  394.63   2.94         33.4  \n",
      "4     18.7  396.90   5.33         36.2  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yikaima/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "boston_dataset = datasets.load_boston()\n",
    "\n",
    "boston_pd = pd.DataFrame(boston_dataset.data)\n",
    "boston_pd.columns = boston_dataset.feature_names\n",
    "boston_pd_target = np.asarray(boston_dataset.target)\n",
    "boston_pd['House Price'] = pd.Series(boston_pd_target)\n",
    "\n",
    "X = boston_pd.iloc[:,:-1]\n",
    "y = boston_pd.iloc[:,-1]\n",
    "\n",
    "print(boston_pd.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aedae85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape of X_train is (379, 13) and the shape of Y_train is (379,) \n",
      "Train data shape of X_test is (127, 13) and the shape of Y_test is (127,) \n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(boston_pd.iloc[:,:-1],boston_pd.iloc[:,-1],test_size=0.25)\n",
    "\n",
    "print(\"Train data shape of X_train is {} and the shape of Y_train is {} \".format(x_train.shape,y_train.shape))\n",
    "\n",
    "print(\"Train data shape of X_test is {} and the shape of Y_test is {} \".format(x_test.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c52c6a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on test set is:  18.46406761968643\n"
     ]
    }
   ],
   "source": [
    "# Apply multiple Linear Regression Model\n",
    "lreg = LinearRegression()\n",
    "lreg.fit(x_train,y_train)\n",
    "\n",
    "lreg_y_pred = lreg.predict(x_test)\n",
    "\n",
    "# Calculate the Mean Squared Error (MSE)\n",
    "mean_square_error = np.mean( (lreg_y_pred - y_test)**2)\n",
    "print(\"MSE on test set is: \",mean_square_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5048adeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Columns  Coefficient Estimate\n",
      "0      CRIM             -0.115707\n",
      "1        ZN              0.049178\n",
      "2     INDUS              0.064928\n",
      "3      CHAS              3.682408\n",
      "4       NOX            -21.229517\n",
      "5        RM              3.295127\n",
      "6       AGE              0.008618\n",
      "7       DIS             -1.602754\n",
      "8       RAD              0.360581\n",
      "9       TAX             -0.015245\n",
      "10  PTRATIO             -0.965489\n",
      "11        B              0.007418\n",
      "12    LSTAT             -0.601118\n"
     ]
    }
   ],
   "source": [
    "# Putting together the coefficient and their corresponding variable names\n",
    "lreg_coefficient = pd.DataFrame()\n",
    "lreg_coefficient[\"Columns\"] = x_train.columns\n",
    "lreg_coefficient[\"Coefficient Estimate\"] = pd.Series(lreg.coef_)\n",
    "print(lreg_coefficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c356668c",
   "metadata": {},
   "source": [
    "### Now we want to reduce the coefficient score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad189a0a",
   "metadata": {},
   "source": [
    "### 1. Ridge (L2) score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0d36494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE is : 17.973915218391458\n",
      "    Columns  Coefficient Estimate\n",
      "0      CRIM             -0.109326\n",
      "1        ZN              0.050946\n",
      "2     INDUS              0.025695\n",
      "3      CHAS              3.376115\n",
      "4       NOX            -11.021863\n",
      "5        RM              3.367979\n",
      "6       AGE              0.000203\n",
      "7       DIS             -1.439719\n",
      "8       RAD              0.344392\n",
      "9       TAX             -0.016490\n",
      "10  PTRATIO             -0.855682\n",
      "11        B              0.007915\n",
      "12    LSTAT             -0.618737\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridgeR = Ridge(alpha = 1)\n",
    "ridgeR.fit(x_train,y_train)\n",
    "y_pred = ridgeR.predict(x_test)\n",
    "\n",
    "# calculate the mean squared error\n",
    "mean_squared_error_ridge = np.mean((y_pred - y_test)**2)\n",
    "print(\"The MSE is :\",mean_squared_error_ridge )\n",
    "\n",
    "# get the ridge coefficient and print them\n",
    "ridge_coefficient = pd.DataFrame()\n",
    "ridge_coefficient[\"Columns\"] = x_train.columns\n",
    "ridge_coefficient['Coefficient Estimate'] = pd.Series(ridgeR.coef_)\n",
    "print(ridge_coefficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68c2dda",
   "metadata": {},
   "source": [
    "### 2. Lasso (R1) Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2f53b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE is:  23.556117175525216\n",
      "    Columns  Coefficient Estimate\n",
      "0      CRIM             -0.074297\n",
      "1        ZN              0.054410\n",
      "2     INDUS             -0.000000\n",
      "3      CHAS              0.000000\n",
      "4       NOX             -0.000000\n",
      "5        RM              0.443685\n",
      "6       AGE              0.030163\n",
      "7       DIS             -0.733494\n",
      "8       RAD              0.327972\n",
      "9       TAX             -0.018220\n",
      "10  PTRATIO             -0.681699\n",
      "11        B              0.006557\n",
      "12    LSTAT             -0.862328\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha = 1)\n",
    "lasso.fit(x_train,y_train)\n",
    "y_pred = lasso.predict(x_test)\n",
    "\n",
    "mean_squared_error_lasso = np.mean((y_pred - y_test)** 2)\n",
    "print(\"The MSE is: \", mean_squared_error_lasso)\n",
    "\n",
    "lasso_coefficient = pd.DataFrame()\n",
    "lasso_coefficient[\"Columns\"] = x_train.columns\n",
    "lasso_coefficient[\"Coefficient Estimate\"] = pd.Series(lasso.coef_)\n",
    "print(lasso_coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3007135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
