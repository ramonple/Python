{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07bee5a7",
   "metadata": {},
   "source": [
    "#### 1. Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a5f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform group by on 'continent' and find the mean and median of 'wine_serving'\n",
    "\n",
    "data.groupby(['continent'])['wine_serving'].agg([np.mean,np.median])) # DON't forget the [] in agg([])\n",
    "\n",
    "# Calculate total co2_emission per country: emissions_by_country\n",
    "emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb5fc54",
   "metadata": {},
   "source": [
    "#### 2. Find the Subsection of a DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39b2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset for food_category equals rice\n",
    "\n",
    "rice_consumption = food_consumption[food_consumption['food_category'] == 'rice'] # NOTICE: all []s\n",
    "\n",
    "# we need to call the df two times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2779a38",
   "metadata": {},
   "source": [
    "#### 3. Plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3abcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y) # line plot\n",
    "\n",
    "plt.hist(x, bins =  )\n",
    "\n",
    "plt.scatter(x,y)\n",
    "\n",
    "# more complex condition:\n",
    "\n",
    "# Create a histogram of restaurant_groups and show plot\n",
    "restaurant_groups['group_size'].hist( bins = [2,3,4,5,6])\n",
    "\n",
    "# Create histogram of co2_emission for food_category 'beef'\n",
    "food_consumption[food_consumption['food_category'] == 'beef']['co2_emission'].hist() # notice the []s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bb952c",
   "metadata": {},
   "source": [
    "#### 4. Quantile & Percential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a639d629",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.quantitle(df['xx'],0.5)\n",
    "np.percentile(df['xx'],50)\n",
    "# or \n",
    "np.quantile(df['xx'],[0,0.2,0.4,0.6,0.8])\n",
    "\n",
    "# IQR:\n",
    "iqr = np.quantile(df['xx'],0.75) - np.quantile(df['xx'], 0.25)\n",
    "\n",
    "# Calculate the six quantiles that split up the data into 5 pieces (quintiles) \n",
    "# of the co2_emission column of food_consumption\n",
    "print(np.quantile(food_consumption['co2_emission'], [0, 0.2, 0.4, 0.6, 0.8, 1])) # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3517cb99",
   "metadata": {},
   "source": [
    "#### 5. linspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b98b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(start, stop, num)\n",
    "# num: Number of samples to generate\n",
    "\n",
    "np.quantile(df['xx'],np.linspace(a,b,num))\n",
    "print(np.quantile(food_consumption['co2_emission'], np.linspace(0,1,6))) \n",
    "# that is [0, 0.2, 0.4, 0.6, 0.8, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c026c7d9",
   "metadata": {},
   "source": [
    "#### 5. finding outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dc190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import iqr\n",
    "\n",
    "iqr = iqr(df['xxx'])\n",
    "iqr = q3 - q1 # q1 = np.quantile(df['xx'], 0.25)\n",
    "\n",
    "lower_threshold = np.quantile(df['xx'], 0.25) - iqr * 1.5\n",
    "upper_threshold = np.quantile(df['xx'],0.75) + iqr * 1.5\n",
    "\n",
    "df [ (df['xx'] < lower_threshold) | (df['xx'] > upper_threshold)]\n",
    "\n",
    "# !! Subset emissions_by_country to find outliers\n",
    "outliers = emissions_by_country[(emissions_by_country < lower) | (emissions_by_country > upper)] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f82d0ca",
   "metadata": {},
   "source": [
    "#### 6. get a sample from a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242efdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(num)\n",
    "df['xxxx'].sample(num)\n",
    "\n",
    "df.sample(num, replace = True/ False)\n",
    "# num: how many samples we want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05f8f13",
   "metadata": {},
   "source": [
    "#### 7. get random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9348e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff12599",
   "metadata": {},
   "source": [
    "#### 8. count( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0e1e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the deals for each product\n",
    "counts = amir_deals['product'].value_counts()\n",
    "\n",
    "# Function explainations:\n",
    "Series.value_counts() # return a series containing counts of [unique] values\n",
    "np.count_nonzero()\n",
    "np.unique( ) # find the unique element of an array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e086e52",
   "metadata": {},
   "source": [
    "#### 9. Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25cadb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probability of picking a deal with each product\n",
    "counts = amir_deals['product'].value_counts()\n",
    "probs = counts / amir_deals.shape[0]\n",
    "\n",
    "# You can get the number of rows in a DataFrame using the .shape[0]\n",
    "# Y.shape is (n,m)\n",
    "# Y.shape[0] =n, Y.shape[1] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907a6d06",
   "metadata": {},
   "source": [
    "#### 10. creating probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a6256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create probability distribution\n",
    "size_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups['group_size'].shape[0]\n",
    "\n",
    "# Reset index and rename columns\n",
    "size_dist = size_dist.reset_index()\n",
    "size_dist.columns = ['group_size','prob' ]\n",
    "\n",
    "# Expected value\n",
    "# Calculate the expected value of the size_distribution, which represents the expected group size, \n",
    "# by multiplying the group_size by the prob and taking the sum.\n",
    "expected_value = np.sum(size_dist['group_size'] * size_dist['prob'])\n",
    "print(expected_value)\n",
    "\n",
    "# size_dist:\n",
    "<script.py> output:\n",
    "       group_size  prob\n",
    "    0           2   0.6\n",
    "    1           4   0.2\n",
    "    2           6   0.1\n",
    "    3           3   0.1\n",
    "\n",
    "# Subset groups of size 4 or more\n",
    "groups_4_or_more = size_dist[size_dist['group_size'] >= 4] # should be: df[df['xx'] condition], all []\n",
    "\n",
    "# Sum the probabilities of groups_4_or_more\n",
    "prob_4_or_more = np.sum(groups_4_or_more['prob'])\n",
    "print(prob_4_or_more)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe917890",
   "metadata": {},
   "source": [
    "### 11. Scipy.stats\n",
    "\n",
    "This module contains a large number of probability distributions, summary and frequency statistics, correlation functions and statistical tests, masked statistics, kernel density estimation, quasi-Monte Carlo functionality, and more.\n",
    "\n",
    "\n",
    "#### probability distribution\n",
    "\n",
    "rv_continuous: A generic continuous random variable class meant for subclassing.\n",
    "\n",
    "rv_discrete: A generic discrete random variable class meant for subclassing.\n",
    "\n",
    "rv_histogram: Generates a distribution given by a histogram.\n",
    "\n",
    "#### continuous distributions\n",
    "\n",
    "scipy.stats.distribution_name. rvs/pdf/cdf\n",
    "\n",
    "\n",
    "rvs: random variables\n",
    "\n",
    "pdf(x,loc=,scale=) \n",
    "\n",
    "cdf(x,loc=,scale=)\n",
    "\n",
    "\n",
    "chi2\n",
    "\n",
    "expon\n",
    "\n",
    "norm\n",
    "\n",
    "pareto\n",
    "\n",
    "t\n",
    "\n",
    "uniform\n",
    "\n",
    "weibull_min/ weibull_max\n",
    "\n",
    "##### Other functions:\n",
    "\n",
    "\n",
    "#### discrete distribution\n",
    "\n",
    "scipy.stats.distribution_name.pmf/cdf       # pmf: prbability mass function\n",
    "\n",
    "bernoulli\n",
    "\n",
    "binom\n",
    "\n",
    "randint\n",
    "\n",
    "#### summary statistics\n",
    "describe\n",
    "\n",
    "kurtosis\n",
    "\n",
    "mode\n",
    "\n",
    "skew\n",
    "\n",
    "entropy\n",
    "\n",
    "\n",
    "#### correlation functions\n",
    "\n",
    "pearson\n",
    "\n",
    "#### statistical tests\n",
    "\n",
    "ttest_lsamp  # T-test for one group of scores\n",
    "\n",
    "ttest_ind   # T-test for the means of two independent samples of scores\n",
    "\n",
    "chisquare\n",
    "\n",
    "wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d28d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Uniform\n",
    "from scipy.stats import uniform\n",
    "\n",
    "uniform.rvs(min, max, size = number_of_rvs)\n",
    "uniform.cdf(target_threshold, min, max)\n",
    "\n",
    "\n",
    "####### Binomial\n",
    "from scipy.stats import binom\n",
    "binom.rvs(n,p,size =)\n",
    "binom.pmf(k,n,p,loc = )\n",
    "binom.pmf(k,n,p,loc= )\n",
    "\n",
    "\n",
    "###### Nomral Distribution\n",
    "from scipy.stats import norm\n",
    "norm.rvs(loc =, scale =, size =, random_state =) ## Notice the sequence !!! and 'size = '\n",
    "norm.pdf(x, loc= , scale =)\n",
    "norm.cdf(x,loc = , scale = )\n",
    "norm.ppf(q,loc=, scale)   # percent point function\n",
    "\n",
    "\n",
    "###### Poisson Distribution:\n",
    "# Discrete \n",
    "# probability of a give number of events occuring in a fixed time interval at a certain rate\n",
    "# lambda = average number of events per time interval\n",
    "from scipy.stats import poisson\n",
    "poisson.rvs(lambda,loc= ,size=, random_seed= )\n",
    "poisson.pmf(k,lambda,loc)\n",
    "poisson.cdf(k,lambda,loc)\n",
    "\n",
    "\n",
    "###### Exponential Distribution\n",
    "# time between events in a Poisson Process\n",
    "# scale = 1/lambda; lambda: rate (poisson)\n",
    "from scipy.stats import expon\n",
    "expon.rvs(loc = , scale=, size=)\n",
    "expon.pdf(x,loc=, scale=) # must print 'loc = / scale = '\n",
    "expon.cdf(x,loc=,scale=)\n",
    "expon.ppf(q,loc=,scale=) \n",
    "\n",
    "\n",
    "######## t distribution\n",
    "#  The degrees of freedom refers to the number of independent observations in a set of data.\n",
    "from scipy.stats import t\n",
    "t.rvs(df, loc=, scale=, size=)\n",
    "t.pdf(x,df,loc= scale=)\n",
    "t.cdf(x, df, loc =, scale = )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e443fd",
   "metadata": {},
   "source": [
    "#### 12. Central Limit Theorem\n",
    "\n",
    "The sampling distribution of a statistic becomes closwer to the Normal distribution as the number of trials increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1727bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example:\n",
    "\n",
    "sample_means = []\n",
    "\n",
    "# Loop 100 times\n",
    "for i in range(100):\n",
    "  # Take sample of 20 num_users\n",
    "  samp_20 = amir_deals['num_users'].sample(20, replace=True)\n",
    "  # Calculate mean of samp_20\n",
    "  samp_20_mean = np.mean(samp_20)\n",
    "  # Append samp_20_mean to sample_means\n",
    "  sample_means.append(samp_20_mean)\n",
    "  \n",
    "# Convert to Series and plot histogram !!! this is very important\n",
    "sample_means_series = pd.Series(sample_means)\n",
    "sample_means_series.hist()\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1875f2d",
   "metadata": {},
   "source": [
    "#### 13. Correlation\n",
    "\n",
    "ONLY measures linear relationship\n",
    "\n",
    "Magnitude of correlation: strength of relationship\n",
    "\n",
    "Sign of correlation: direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd93b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing correlation\n",
    "df['xxx'].corr(df['yyy'])\n",
    "\n",
    "import seaborn as sns\n",
    "sns.scatterplot(x=,y=, hue=, style=, data=, pallete=, ci=)\n",
    "sns.scatterplot(x='',y='', data = df)  # needs '' for column names, Does not need '' for dataset\\\n",
    "\n",
    "# ci: int or 'sd' or None, size of confidence interval. 'sd' means draw std of the data\n",
    "# Notice: it is scatterplot NOT scatter\n",
    "\n",
    "sns.lmplot(x=,y=,data=,hue=,...)\n",
    "# plot data and regression model fits across a FacetGrid\n",
    "\n",
    "\n",
    "##### Transformations:\n",
    "# log()\n",
    "# sqrt()\n",
    "# 1/x\n",
    "\n",
    "df['log_xxx'] = np.log(df['xxx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c80b76",
   "metadata": {},
   "source": [
    "#### 14. Confounders\n",
    "\n",
    "In statistics, a confounder (also confounding variable, confounding factor, extraneous determinant or lurking variable) is a variable that influences both the dependent variable and independent variable, causing a spurious association.\n",
    "\n",
    "\n",
    "\n",
    "##### Design of experiments\n",
    "\n",
    "Controlled experiments: Treatment Group, Contral Group\n",
    "\n",
    "##### Longitudinal Study:\n",
    "\n",
    "* Partipicants are followed over a period of time to examine effect of treatment on response.\n",
    "\n",
    "* Effect of age on height not confounded by generation\n",
    "\n",
    "* More expensive, results take longer\n",
    "\n",
    "##### Cross-sectional Study\n",
    "\n",
    "* Data on participants is collected from a singe snapshot in time\n",
    "\n",
    "* Effect of age on height is confounded by generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e265e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
