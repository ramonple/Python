{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07bee5a7",
   "metadata": {},
   "source": [
    "#### 1. Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a5f8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform group by on 'continent' and find the mean and median of 'wine_serving'\n",
    "\n",
    "data.groupby(['continent'])['wine_serving'].agg([np.mean,np.median])) # DON't forget the [] in agg([])\n",
    "\n",
    "# Calculate total co2_emission per country: emissions_by_country\n",
    "emissions_by_country = food_consumption.groupby('country')['co2_emission'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb5fc54",
   "metadata": {},
   "source": [
    "#### 2. Find the Subsection of a DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39b2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset for food_category equals rice\n",
    "\n",
    "rice_consumption = food_consumption[food_consumption['food_category'] == 'rice'] # NOTICE: all []s\n",
    "\n",
    "# we need to call the df two times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2779a38",
   "metadata": {},
   "source": [
    "#### 3. Plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3abcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y) # line plot\n",
    "\n",
    "plt.hist(x, bins =  )\n",
    "\n",
    "plt.scatter(x,y)\n",
    "\n",
    "# more complex condition:\n",
    "\n",
    "# Create a histogram of restaurant_groups and show plot\n",
    "restaurant_groups['group_size'].hist( bins = [2,3,4,5,6])\n",
    "\n",
    "# Create histogram of co2_emission for food_category 'beef'\n",
    "food_consumption[food_consumption['food_category'] == 'beef']['co2_emission'].hist() # notice the []s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bb952c",
   "metadata": {},
   "source": [
    "#### 4. Quantile & Percential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a639d629",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.quantitle(df['xx'],0.5)\n",
    "np.percentile(df['xx'],50)\n",
    "# or \n",
    "np.quantile(df['xx'],[0,0.2,0.4,0.6,0.8])\n",
    "\n",
    "# IQR:\n",
    "iqr = np.quantile(df['xx'],0.75) - np.quantile(df['xx'], 0.25)\n",
    "\n",
    "# Calculate the six quantiles that split up the data into 5 pieces (quintiles) \n",
    "# of the co2_emission column of food_consumption\n",
    "print(np.quantile(food_consumption['co2_emission'], [0, 0.2, 0.4, 0.6, 0.8, 1])) # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3517cb99",
   "metadata": {},
   "source": [
    "#### 5. linspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b98b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(start, stop, num)\n",
    "# num: Number of samples to generate\n",
    "\n",
    "np.quantile(df['xx'],np.linspace(a,b,num))\n",
    "print(np.quantile(food_consumption['co2_emission'], np.linspace(0,1,6))) \n",
    "# that is [0, 0.2, 0.4, 0.6, 0.8, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c026c7d9",
   "metadata": {},
   "source": [
    "#### 5. finding outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dc190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import iqr\n",
    "\n",
    "iqr = iqr(df['xxx'])\n",
    "iqr = q3 - q1 # q1 = np.quantile(df['xx'], 0.25)\n",
    "\n",
    "lower_threshold = np.quantile(df['xx'], 0.25) - iqr * 1.5\n",
    "upper_threshold = np.quantile(df['xx'],0.75) + iqr * 1.5\n",
    "\n",
    "df [ (df['xx'] < lower_threshold) | (df['xx'] > upper_threshold)]\n",
    "\n",
    "# !! Subset emissions_by_country to find outliers\n",
    "outliers = emissions_by_country[(emissions_by_country < lower) | (emissions_by_country > upper)] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f82d0ca",
   "metadata": {},
   "source": [
    "#### 6. get a sample from a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242efdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(num)\n",
    "df['xxxx'].sample(num)\n",
    "\n",
    "df.sample(num, replace = True/ False)\n",
    "# num: how many samples we want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05f8f13",
   "metadata": {},
   "source": [
    "#### 7. get random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9348e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff12599",
   "metadata": {},
   "source": [
    "#### 8. count( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0e1e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the deals for each product\n",
    "counts = amir_deals['product'].value_counts()\n",
    "\n",
    "# Function explainations:\n",
    "Series.value_counts() # return a series containing counts of [unique] values\n",
    "np.count_nonzero()\n",
    "np.unique( ) # find the unique element of an array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e086e52",
   "metadata": {},
   "source": [
    "#### 9. Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25cadb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probability of picking a deal with each product\n",
    "counts = amir_deals['product'].value_counts()\n",
    "probs = counts / amir_deals.shape[0]\n",
    "\n",
    "# You can get the number of rows in a DataFrame using the .shape[0]\n",
    "# Y.shape is (n,m)\n",
    "# Y.shape[0] =n, Y.shape[1] = m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907a6d06",
   "metadata": {},
   "source": [
    "#### 10. creating probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a6256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create probability distribution\n",
    "size_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups['group_size'].shape[0]\n",
    "\n",
    "# Reset index and rename columns\n",
    "size_dist = size_dist.reset_index()\n",
    "size_dist.columns = ['group_size','prob' ]\n",
    "\n",
    "# Expected value\n",
    "# Calculate the expected value of the size_distribution, which represents the expected group size, \n",
    "# by multiplying the group_size by the prob and taking the sum.\n",
    "expected_value = np.sum(size_dist['group_size'] * size_dist['prob'])\n",
    "print(expected_value)\n",
    "\n",
    "# size_dist:\n",
    "<script.py> output:\n",
    "       group_size  prob\n",
    "    0           2   0.6\n",
    "    1           4   0.2\n",
    "    2           6   0.1\n",
    "    3           3   0.1\n",
    "\n",
    "# Subset groups of size 4 or more\n",
    "groups_4_or_more = size_dist[size_dist['group_size'] >= 4] # should be: df[df['xx'] condition], all []\n",
    "\n",
    "# Sum the probabilities of groups_4_or_more\n",
    "prob_4_or_more = np.sum(groups_4_or_more['prob'])\n",
    "print(prob_4_or_more)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293a3bc3",
   "metadata": {},
   "source": [
    "### 11. ECDF: empirical cumulative distribution function\n",
    "\n",
    "An empirical distribution function provides a way to model and sample cumulative probabilities for a data sample that does not fit a standard probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60862334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "ecdf = ECDF(sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe917890",
   "metadata": {},
   "source": [
    "### 12. Scipy.stats\n",
    "\n",
    "This module contains a large number of probability distributions, summary and frequency statistics, correlation functions and statistical tests, masked statistics, kernel density estimation, quasi-Monte Carlo functionality, and more.\n",
    "\n",
    "\n",
    "#### probability distribution\n",
    "\n",
    "rv_continuous: A generic continuous random variable class meant for subclassing.\n",
    "\n",
    "rv_discrete: A generic discrete random variable class meant for subclassing.\n",
    "\n",
    "rv_histogram: Generates a distribution given by a histogram.\n",
    "\n",
    "#### continuous distributions\n",
    "\n",
    "scipy.stats.distribution_name. rvs/pdf/cdf\n",
    "\n",
    "\n",
    "rvs: random variables\n",
    "\n",
    "pdf(x,loc=,scale=) \n",
    "\n",
    "cdf(x,loc=,scale=)\n",
    "\n",
    "\n",
    "chi2\n",
    "\n",
    "expon\n",
    "\n",
    "norm\n",
    "\n",
    "pareto\n",
    "\n",
    "t\n",
    "\n",
    "uniform\n",
    "\n",
    "weibull_min/ weibull_max\n",
    "\n",
    "##### Other functions:\n",
    "\n",
    "\n",
    "#### discrete distribution\n",
    "\n",
    "scipy.stats.distribution_name.pmf/cdf       # pmf: prbability mass function\n",
    "\n",
    "bernoulli\n",
    "\n",
    "binom\n",
    "\n",
    "randint\n",
    "\n",
    "#### summary statistics\n",
    "describe\n",
    "\n",
    "kurtosis\n",
    "\n",
    "mode\n",
    "\n",
    "skew\n",
    "\n",
    "entropy\n",
    "\n",
    "\n",
    "#### correlation functions\n",
    "\n",
    "pearson\n",
    "\n",
    "#### statistical tests\n",
    "\n",
    "ttest_lsamp  # T-test for one group of scores\n",
    "\n",
    "ttest_ind   # T-test for the means of two independent samples of scores\n",
    "\n",
    "chisquare\n",
    "\n",
    "wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ebb8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random numbers\n",
    "\n",
    "# random choice\n",
    "np.random.choice(target_string, number_extract, replace = True/False)\n",
    "\n",
    "# from normal distribution\n",
    "np.random.normal(loc = ,scale =, size= )\n",
    "\n",
    "# from poisson distribution\n",
    "np.random.poisson(lambda = , size = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d28d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Uniform\n",
    "from scipy.stats import uniform\n",
    "\n",
    "uniform.rvs(min, max, size = number_of_rvs)\n",
    "uniform.cdf(target_threshold, min, max)\n",
    "\n",
    "\n",
    "####### Binomial\n",
    "from scipy.stats import binom\n",
    "r.rvs(n,p,size =)\n",
    "binom.pmf(k,n,p,loc = )\n",
    "binom.pmf(k,n,p,loc= )\n",
    "\n",
    "\n",
    "###### Nomral Distribution\n",
    "from scipy.stats import norm\n",
    "norm.rvs(loc =, scale =, size =, random_state =) ## Notice the sequence !!! and 'size = '\n",
    "norm.pdf(x, loc= , scale =)\n",
    "norm.cdf(x,loc = , scale = )\n",
    "norm.ppf(q,loc=, scale)   # percent point function\n",
    "\n",
    "\n",
    "###### Poisson Distribution:\n",
    "# Discrete \n",
    "# probability of a give number of events occuring in a fixed time interval at a certain rate\n",
    "# lambda = average number of events per time interval\n",
    "from scipy.stats import poisson\n",
    "poisson.rvs(lambda,loc= ,size=, random_seed= )\n",
    "poisson.pmf(k,lambda,loc)\n",
    "poisson.cdf(k,lambda,loc)\n",
    "\n",
    "\n",
    "###### Exponential Distribution\n",
    "# time between events in a Poisson Process\n",
    "# scale = 1/lambda; lambda: rate (poisson)\n",
    "from scipy.stats import expon\n",
    "expon.rvs(loc = , scale=, size=)\n",
    "expon.pdf(x,loc=, scale=) # must print 'loc = / scale = '\n",
    "expon.cdf(x,loc=,scale=)\n",
    "expon.ppf(q,loc=,scale=) \n",
    "\n",
    "\n",
    "######## t distribution\n",
    "#  The degrees of freedom refers to the number of independent observations in a set of data.\n",
    "from scipy.stats import t\n",
    "t.rvs(df, loc=, scale=, size=)\n",
    "t.pdf(x,df,loc= scale=)\n",
    "t.cdf(x, df, loc =, scale = )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e443fd",
   "metadata": {},
   "source": [
    "#### 13. Central Limit Theorem\n",
    "\n",
    "The sampling distribution of a statistic becomes closwer to the Normal distribution as the number of trials increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1727bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example:\n",
    "\n",
    "sample_means = []\n",
    "\n",
    "# Loop 100 times\n",
    "for i in range(100):\n",
    "  # Take sample of 20 num_users\n",
    "  samp_20 = amir_deals['num_users'].sample(20, replace=True)\n",
    "  # Calculate mean of samp_20\n",
    "  samp_20_mean = np.mean(samp_20)\n",
    "  # Append samp_20_mean to sample_means\n",
    "  sample_means.append(samp_20_mean)\n",
    "  \n",
    "# Convert to Series and plot histogram !!! this is very important\n",
    "sample_means_series = pd.Series(sample_means)\n",
    "sample_means_series.hist()\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1875f2d",
   "metadata": {},
   "source": [
    "#### 14. Correlation\n",
    "\n",
    "ONLY measures linear relationship\n",
    "\n",
    "Magnitude of correlation: strength of relationship\n",
    "\n",
    "Sign of correlation: direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd93b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing correlation\n",
    "df['xxx'].corr(df['yyy'])\n",
    "\n",
    "import seaborn as sns\n",
    "sns.scatterplot(x=,y=, hue=, style=, data=, pallete=, ci=)\n",
    "sns.scatterplot(x='',y='', data = df)  # needs '' for column names, Does not need '' for dataset\\\n",
    "\n",
    "# ci: int or 'sd' or None, size of confidence interval. 'sd' means draw std of the data\n",
    "# Notice: it is scatterplot NOT scatter\n",
    "\n",
    "sns.lmplot(x=,y=,data=,hue=,...)\n",
    "# plot data and regression model fits across a FacetGrid\n",
    "\n",
    "\n",
    "##### Transformations:\n",
    "# log()\n",
    "# sqrt()\n",
    "# 1/x\n",
    "\n",
    "df['log_xxx'] = np.log(df['xxx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c80b76",
   "metadata": {},
   "source": [
    "#### 15. Confounders\n",
    "\n",
    "In statistics, a confounder (also confounding variable, confounding factor, extraneous determinant or lurking variable) is a variable that influences both the dependent variable and independent variable, causing a spurious association.\n",
    "\n",
    "\n",
    "\n",
    "##### Design of experiments\n",
    "\n",
    "Controlled experiments: Treatment Group, Contral Group\n",
    "\n",
    "##### Longitudinal Study:\n",
    "\n",
    "* Partipicants are followed over a period of time to examine effect of treatment on response.\n",
    "\n",
    "* Effect of age on height not confounded by generation\n",
    "\n",
    "* More expensive, results take longer\n",
    "\n",
    "##### Cross-sectional Study\n",
    "\n",
    "* Data on participants is collected from a singe snapshot in time\n",
    "\n",
    "* Effect of age on height is confounded by generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43d0eae",
   "metadata": {},
   "source": [
    "## Hypothesis Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8f8afd",
   "metadata": {},
   "source": [
    "### Bootstrap distribution\n",
    "\n",
    "Bootstrapping is a method that estimates the sampling distribution by taking multiple samples with replacement from a single random sample. \n",
    "\n",
    "These repeated samples are called resamples. Each resample is the same size as the original sample. \n",
    "\n",
    "The advantages of bootstrapping are that it is a straightforward way to derive the estimates of standard errors and confidence intervals, and it is convenient since it avoids the cost of repeating the experiment to get other groups of sampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad1c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a boostrap distribution (dataset ='df', column name  =' target_column')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Step 3: repeat steps 1 & 2 many time, appending a list\n",
    "so_boot_distn =[]\n",
    "\n",
    "for i in range(1000):\n",
    "    so_boot_distn.append(\n",
    "    # step 2: Calculate point estimate\n",
    "    np.mean (\n",
    "           # step 1: Resample\n",
    "           df.sample(frac = 1, replace = True)['target_column']\n",
    "         )\n",
    "    )\n",
    "\n",
    "# standard error\n",
    "std_error = np.std(so_boot_distn, ddof = 1) # must includes [ddof], ddof: Delta Degrees of Freedom.\n",
    "\n",
    "# Z -value: (sample stat - hypoth_para_value) / standard - error\n",
    "sample_mean = df['target_column'].mean()\n",
    "hpyo_mean = 0.5/1/2/,...\n",
    "\n",
    "Z_score = (sample_mean - hypo_mean) / std_error\n",
    "\n",
    "# Calculate the p-value from the z score, for example normal distribution\n",
    "p_value = 1 - norm.cdf(z_score,loc=0,scale=1)\n",
    "\n",
    "# Confidence Interval\n",
    "lower = np.quantile(first_code_boot_disn,0.025)\n",
    "upper = np.quantile(first_code_boot_disn, 0.975)\n",
    "print((lower,upper))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecca6b4",
   "metadata": {},
   "source": [
    "### Type I & Type II Error\n",
    "\n",
    "Type I : False Positive -> reject when H0 is True\n",
    "\n",
    "Type II: False Negative _> Fails to reject when H0 is False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1958f0",
   "metadata": {},
   "source": [
    "## Four Types of Test Statistics\n",
    "\n",
    "* T test\n",
    "\n",
    "* Z Test\n",
    "\n",
    "* ANONVA Test\n",
    "\n",
    "* Chi-Square Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dce239",
   "metadata": {},
   "source": [
    "### Pingouin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35660415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin\n",
    "pingouin.ttest(x = , y = , paired = True/False, alternative ='two-sided'/'less'/'greater', confident = 0.95) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39563b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pingouin import ttest\n",
    "\n",
    "# one -sample t test\n",
    "ttest(x, number)\n",
    "\n",
    "# two samples unpaired t test\n",
    "ttest(SampleA, SampleB, paired = False, alternative ='two-sided'/'less'/'greater')\n",
    "\n",
    "# two samples paired t test\n",
    "ttest(pre, post, paired = True, alternative ='two-sided'/'less'/'greater')\n",
    "\n",
    "##########################################################################################\n",
    "# when we have more than two groups:\n",
    "\n",
    "# ANOVA TEST\n",
    "pingouin.annoca(data = , dv = '', between ='')\n",
    "# dv: Name of column in data containing the dependent variable.\n",
    "# between: Name of column(s) in data containing the between-subject factor(s).\n",
    "\n",
    "# pairwise ttest\n",
    "pingouin.pairwise_tests(data = , dv ='', between='', padjust = )\n",
    "# padjust: Method used for testing and adjustment of pvalues.\n",
    "# 'none','bonf','sidak'....."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5998349e",
   "metadata": {},
   "source": [
    "### T Test\n",
    "\n",
    "Using a sample standard deviation to estimate the standard error is computationally easier than using bootstrapping. However, to correct for the approximation, you need to use a t-distribution when transforming the test statistic\n",
    "\n",
    "T Test has 2 types: 1. one sampled t test 2. two sampled t test\n",
    "\n",
    "#### Fat-tails: \n",
    "\n",
    "T distributions have a greater chance for extreme values than normal distributions, hence the fatter tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d98d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "1 - t.cdf(t_stat, df = degree_of_freedom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82680b9a",
   "metadata": {},
   "source": [
    "#### one sample t test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4d7d62",
   "metadata": {},
   "source": [
    "scipy.stats.ttest_1samp(a, population_mean)\n",
    "\n",
    "* Calculate the T-test for the mean of ONE group of scores.\n",
    "\n",
    "* returns: t-statistic, p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f30d2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "tset, pval = ttest_1samp (data, test_average_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121e26a9",
   "metadata": {},
   "source": [
    "#### two sampled t test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb4d55f",
   "metadata": {},
   "source": [
    "##### Unpaired\n",
    "\n",
    "scipy.stats.ttest_ind(a,b, equal_var = True/False)\n",
    "\n",
    "* Calculate the T-test for the means of two independent samples of scores.\n",
    "\n",
    "##### Paired\n",
    "\n",
    "scipy.stats.ttest_rel(a,b)\n",
    "\n",
    "* Calculate the t-test on TWO RELATED samples of scores, a and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b33e97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "ttest,pval = ttest_ind(a,b)\n",
    "\n",
    "# paired t test\n",
    "ttest,pval = ttest_rel(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e0e2e5",
   "metadata": {},
   "source": [
    "### Z test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea7f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats import weightstats as stests\n",
    "# test for mean based on normal distribution, one or two samples\n",
    "\n",
    "\n",
    "\n",
    "# one sample z test\n",
    "\n",
    "ztest,pval = stests.ztest(x1 ,x2 = None, value = )\n",
    "# value: In the one sample case, value is the mean of x1 under the Null hypothesis\n",
    "\n",
    "\n",
    "# two sample z test\n",
    "ztest, pval1 = stests.ztest(df1, x2 = df2, value = )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926b0c66",
   "metadata": {},
   "source": [
    "### ANOVA ( F - test)\n",
    "\n",
    "The t-test works well when dealing with two groups, but sometimes we want to compare more than two groups at the same time. \n",
    "\n",
    "F = Between group variability / Within group variability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27a537b",
   "metadata": {},
   "source": [
    "#### One way ANOVA: \n",
    "\n",
    "One independent variable\n",
    "\n",
    "It tell whether two or more groups are similar or not based on their mean similarity and f-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb2b0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "F, p = stats.f_oneway(a,b,c)\n",
    "\n",
    "# returns: f-statitics, p value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebee749c",
   "metadata": {},
   "source": [
    "#### Two way ANOVA:\n",
    "\n",
    "More than 1 independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c0fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples:\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "data_lm = ols('',data=data).fit()\n",
    "\n",
    "table = sm.stats.anova_lm(data, typ=2) # Type 2 ANOVA DataFrame\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851736dc",
   "metadata": {},
   "source": [
    "### Chi-Squared Test\n",
    "\n",
    "The test is applied when you have two categorical variables from a single population.\n",
    "\n",
    "It is used to determine whether there is a significant association between the two variables.\n",
    "\n",
    "#### Contingency Table\n",
    "\n",
    "A contingency table is a tabular representation of categorical data .\n",
    "\n",
    "A contingency table usually shows frequencies for particular combinations of values of two discrete random variable s X and Y. \n",
    "\n",
    "* If Statistic >= Critical Value: significant result, reject null hypothesis (H0), dependent.\n",
    "* If Statistic < Critical Value: not significant result, fail to reject null hypothesis (H0), independent.\n",
    "\n",
    "#### scipy. stats. chi2_contingency\n",
    "\n",
    "##### Parameters:\n",
    "\n",
    "observed\n",
    "\n",
    "correction\n",
    "\n",
    "lambda_\n",
    "\n",
    "#####  Returns:\n",
    "\n",
    "chi2\n",
    "\n",
    "p\n",
    "\n",
    "dof\n",
    "\n",
    "expected: The expected frequencies,\n",
    "\n",
    "##### -------------------------------------\n",
    "\n",
    "Chi-square test of independence of variables in a contingency table.\n",
    "\n",
    "dof = observed.size - sum(observed.shape) + observed.ndim - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f11bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "stats.chisquare(f_obs,f_exp=None, ddof = , axis = )\n",
    "# f_obs: observed frequencies in each category, should be array-like\n",
    "# f_exp: expected frequencies\n",
    "\n",
    "# Returns:\n",
    "# chisq: chi2 test statistic\n",
    "# p value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a4ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, prob, dof, expected = chi2_contengency(table)\n",
    "\n",
    "#### Interpretation\n",
    "\n",
    "prob = 0.95\n",
    "critical = chi2.ppf(prob, dof)\n",
    "if abs(stat) >= critical:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (fail to reject H0)')\n",
    "    \n",
    "    \n",
    "\n",
    "# interpret p-value\n",
    "alpha = 1.0 - prob\n",
    "if p <= alpha:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (fail to reject H0)')  \n",
    "    \n",
    "    \n",
    "\n",
    "# degrees of freedom: (rows - 1) * (cols - 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
