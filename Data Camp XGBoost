RMSE root mean squared error  sqrt(sum(yi-y_pred)^2 / n)
MAE  mean absolute error     |(yi-y_pred)| / n



.iloc[:,:-1]  select all columns without last
.iloc[:,-1]  select the last column



#################################### Course 1: Classification with XGBoost  ####################################
- supervised learning
  AUC: metric for binary classification models
  (area under the ROC curve (x: false positive rate y: true positive rate)
  large AUC  = better model
  
  confusion matrix
  accuracy: tp + tn / (tp + tn + fp + fn)
  
  
  What is XGBoost/
  optimised  gradient-boostting machine learning library
  speed and performance
  core algorithm is parallelizable
  consistently outperforms single-algorithm methods
  
  
  
  import xgbosst as xgb
  import pandas as pd
  import numpy as np
  from  sklearn.model_selection import train_test_split
  
  class_data= pd.read_csv("xxxx")
  X,y=class_data.iloc[:,:-1],class_data.iloc[:,-1]
  X_train,y_train=
  xg_cl= xgb.XGBClassifier(objective ='binary:logistic',n_estimator= 10, seed = 123)
  xg_cl.fit(X_train,y_train)
  preds= xg_cl.predict(X_test)
  
  accuracy = float(np.sum(preds ==y_test))/y_test.shape[0]
  print("accuracy: %f" %(accuracy) )
  
  
  
XGBoost is usually used with trees as base learners
 ( tree is combinations of several binary questions)
individual tree tends to overfit


 

--------  What is Boosting?
At bottom, boosting isn't really a specific machine learning algorithm, but a concept that can be applied to a set of machine learning models. 
So, its really a meta-algorithm. 
Specifically, it is an ensemble meta-algorithm primarily used to reduce any given single learner's variance and to convert many weak learners into an 
arbitrarily strong learner.


Approaches:
 - iteratively learning a set of weak models on subsets of the data
 - weighing each weak prediction according to each weak learner's perfromance
 - combine all weighted predicted to obtain a single weighted prediction


Model evaluation through cross-validation
Cross validation is a robust method for estimating the performance of a model on unseen data

Example:

import xgboost as xgb
import pandas as pd
churn_data = pd.read_csv('xx')
churn_dmatrix = xgb.DMatrix(data=churn_data.iloc[:,:-1],label=churn_data.month_5_still_here)
params = {"objective":"binary:logistic","max_depth":4}
cv_results = xgb.cv(dtrain=churn_dmatrix, params = params, nfold =4, num_boost_round=10,metrics="error",as_pandas = True)
print("Accuracy:%f"%((1-cv_results["test-error-mean"]).iloc[-1]))

## 
Perform n-fold cross-validation by calling xgb.cv(). 
params is your parameter dictionary, 
nfold is the number of cross-validation folds , 
num_boost_round is the number of trees we want to build , 
metrics is the metric you want to compute (this will be "error", which we will convert to an accuracy).



-- When should we use XGBoost
- you have a large number of training samples
    a. Greater than 1000 training samples and less 100 features
    b. the number of features < number of training samples
- you have a mixture of categories and numeric features
    or just numeric features
    

NOT USE:
imagine recognition
computer vision
natural language processing and understanding problems
number of training samples < number of features








#################################### Course 2: Regression with XGBoost  ####################################

Objective (loss) functions and base learners


# Convert the training and testing sets into DMatrixes: DM_train, DM_test
DM_train = xgb.DMatrix(data=X_train, label=y_train)
DM_test =  xgb.DMatrix(data=X_test, label=y_test)

# Create the parameter dictionary: params
params = {"booster":"gblinear", "objective":"reg:linear"}

# Train the model: xg_reg
xg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=5)

# Predict the labels of the test set: preds
preds = xg_reg.predict(DM_test)

# Compute and print the RMSE
rmse = np.sqrt(mean_squared_error(y_test,preds))
print("RMSE: %f" % (rmse))






# Create the DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X,label=y)

# Create the parameter dictionary: params
params = {"objective":"reg:linear", "max_depth":4}



1. 
# Perform cross-validation: cv_results and "rmse"
cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics="rmse", as_pandas=True, seed=123)

# Print cv_results
print(cv_results)

# Extract and print final boosting round metric
print((cv_results["test-rmse-mean"]).tail(1))

2.
# Perform cross-validation: cv_results and  "mae"
cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics="mae", as_pandas=True, seed=123)

# Print cv_results
print(cv_results)

# Extract and print final round boosting round metric
print((cv_results["test-mae-mean"]).tail(1))





----- Regularization and base learners in XGBoost

regularization paramaters in XGBoost
 gamma
 alpha - l1 regularization
 lambda - l2 regularization
 
 
Base learners in XGBoost
linear base learner:
 sum of linear terms
 boosted model is weighted sum of linear models (thus is linear itself)
 rarely used
 
Tree base learner
  decision tree
  Boosted model is weighted sum of decision trees (nonlinear)
  Almost exclusively used in XGBoost
  
  
  
  Creating DataFrames from multiple equal-length lists
  
  pd.DataFrame(list(zip(list1,lists2)),columns=["list1","list2"])
   -- zip creates a generator of parallel values: zip([1,2,3],["a","b","c"]) = [1,"a"],[2,"b"],[3,"c"]
  
  
----- Using regularization in XGBoost  
  
  # Create the DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X, label=y)

reg_params = [1, 10, 100]

# Create the initial parameter dictionary for varying l2 strength: params
params = {"objective":"reg:linear","max_depth":3}

# Create an empty list for storing rmses as a function of l2 complexity
rmses_l2 = []

# Iterate over reg_params
for reg in reg_params:

    # Update l2 strength
    params["lambda"] = reg
    
    # Pass this updated param dictionary into cv
    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix , params=params, nfold=2, num_boost_round=5, metrics="rmse", as_pandas=True, seed=123)
    
    # Append best rmse (final round) to rmses_l2
    rmses_l2.append(cv_results_rmse["test-rmse-mean"].tail(1).values[0])

# Look at best rmse per l2 param
print("Best rmse as a function of l2:")
print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=["l2", "rmse"]))
  
  
  
  
  -------- Visualizing individual XGBoost trees
  
  # Create the DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X, label=y)

# Create the parameter dictionary: params
params = {"objective":"reg:linear", "max_depth":2}

# Train the model: xg_reg
xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)

# Plot the first tree
xgb.plot_tree(xg_reg, num_trees=0)
plt.show()

# Plot the fifth tree
xgb.plot_tree(xg_reg, num_trees=4)
plt.show()

# Plot the last tree sideways
# Plot the last (tenth) tree sideways. To do this, specify the additional keyword argument rankdir="LR"
xgb.plot_tree(xg_reg, num_trees=9, rankdir='LR')
plt.show()
  
  
  
-- Visualizing feature importances: What features are most important in my dataset

  # Create the DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X,label=y)

# Create the parameter dictionary: params
params = {"objective":"reg:linear","max_depth":4}

# Train the model: xg_reg
xg_reg = xgb.train(params=params,dtrain=housing_dmatrix,num_boost_round=10)

# Plot the feature importances
xgb.plot_importance(xg_reg)
plt.show()
  
  
  
  
  
  
  
########################### Course 3: Fine-tuning your XGBoost model ########################################
When is tuning your model a bad idea?
You are very short on time before you must push an initial model to production and have little data to train your model on.





  X,y = housing_data[housing_data.columns.tolist()["-1]], housing_data[housing_data.columns.tolist()[-1]]
   # ()[:-1] It slices the string to omit the last character, this works even on empty strings, it's a pretty safe way of removing that last character
  housing_dmatrix=xgb.DMatrix(data=X,label=y)
  untuned_params = {"objective":"reg:linear"}
  untuned_cv_results_rmse = xgb.cv(dtrain = housing_dmatrix,params=untuned_params,nfold=4,metrics="rmse",as_pandas=True,seed=123)
  print("Untuned rmse: %f" %(untuned_cv_results_rmse["test-rmse-mean"]).tail(1)))
  
  tuned_params = {"objective":"reg:linear",'colsample_bytree':0.3,'learning_rate':0.1,'max_depth':5}
  tuned_cv_results_rmse = xgb.cv(dtrain = housing_dmatrix,params=tuned_params,nfold=4,num_boost_round=200,metrics="rmse",as_pandas=True,seed=123)
  print("Untuned rmse: %f" %(tuned_cv_results_rmse["test-rmse-mean"]).tail(1)))


  # Create your housing DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X,label=y)

# Create the parameter dictionary for each tree: params
params = {"objective":"reg:linear", "max_depth":4}

# Perform cross-validation with early stopping: cv_results
cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=50, early_stopping_rounds=10, metrics="rmse", as_pandas=True, seed=123)

# Print cv_results
print(cv_results)





------------- hyperparameters
common tunable parameters
 learning rate 
 gamma (min loss reduction to create new tree split)
 lambda (l2 reg on leaf weights)
 alpha (l1 reg on leaf weights)
 max_depth
 subsample % smapels used per tree
   # Subsample must be a value between 0 and 1 and is the fraction of the total training set that can be used for any given boosting round. 
     If the value is low, then the fraction of your training data used would per boosting round would be low and you may run into underfitting problems, 
     a value that is very high can lead to overfitting as well.
 colsample_bytree % features used per tree
   # Colsample_bytree is the fraction of features you can select from during any given boosting round and must also be a value between 0 and 1. 
     A large value means that almost all features can be used to build a tree during a given boosting round, whereas a small value means that 
     the fraction of features that can be selected from is very small. 
     In general, smaller colsample_bytree values can be thought of as providing additional regularization to the model, 
     whereas using all columns may in certain cases overfit a trained model.
    
 linear tunable parameters
   lambda l2
   alpha l1
   lamda_bias  l2 reg term on bias
   
   
   
   # Create your housing DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X, label=y)

# Create the parameter dictionary for each tree (boosting round)
params = {"objective":"reg:linear", "max_depth":3}

# Create list of eta values and empty list to store final round rmse per xgboost model
eta_vals = [0.001, 0.01, 0.1]
best_rmse = []

# Systematically vary the eta
for curr_val in eta_vals:

    params["eta"] = curr_val
    
    # Perform cross-validation: cv_results
    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3,
                        num_boost_round=10, early_stopping_rounds=5,
                        metrics="rmse", as_pandas=True, seed=123)
    
    # Append the final round rmse to best_rmse
    best_rmse.append(cv_results["test-rmse-mean"].tail().values[-1])

# Print the resultant DataFrame
print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=["eta","best_rmse"]))


--- Grid Search
from sklearn.model import GridSearchCV

gbm_param_grid={'learning_rate':[0.1,0.2,0.3,0.4],
                'n_estimators':[200],
                'subsample':[0.3,0.5,0.9]}
                
gbm=xgb.XGBRegressor()
grid_mse = GridSearchCV(estimator=gbm,param_grid=gbm_param_grid,scoring='neg_mean_squared_error',cv=4,verbose=1)
grid_mse.fit(X,y) 

print("Best parameter found:", grid_mse.best_params_)
print("Lowest RMSE found:",np.sqrt(np.abs(grid_mse.best_score_)))


--- Random Search
from sklearn.model_selection import RandomizedSearchCV
randomized_mse = RandomizedSearchCV(estimator=gbm, param_distributions=gbm_param_grid,
                                    n_iter=5, scoring='neg_mean_squared_error', cv=4, verbose=1)



# Create the parameter grid: gbm_param_grid 
gbm_param_grid = {
    'n_estimators': [25],
    'max_depth': range(2, 12)
}

# Instantiate the regressor: gbm
gbm = xgb.XGBRegressor(n_estimators=10)

# Perform random search: grid_mse
randomized_mse = RandomizedSearchCV(estimator=gbm, param_distributions=gbm_param_grid,
                                    n_iter=5, scoring='neg_mean_squared_error', cv=4, verbose=1)
randomized_mse.fit(X, y)

# Print the best parameters and lowest RMSE
print("Best parameters found: ",randomized_mse.best_params_)
print("Lowest RMSE found: ", np.sqrt(np.abs(randomized_mse.best_score_)))




########################### Course 4:  Using XGBoost in pipelines ##################################################

take a list of named 2-tupes as inpout


from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocesing import StandardScalar
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score

rf_pipeline = Pipeline[("st_scaler",StandardScalar()),("rf_model",RandomForestRegressor())]
scores = cross_val_score(rf_pipeline,X,y,scoring="neg_mean_squared_error",cv=10)


LabelEncoder : converts a categorical column of strings into integers
OneHotEncoder: takes the column of integers and encodes them as dummy variables




---- Encoding categorical columns I: LabelEncoder

# Import LabelEncoder
from sklearn.preprocessing import LabelEncoder

# Fill missing values with 0
df.LotFrontage = df.LotFrontage.fillna(0)

# Create a boolean mask for categorical columns
categorical_mask = (df.dtypes == object)

# Get list of categorical column names
categorical_columns = df.columns[categorical_mask].tolist()

# Print the head of the categorical columns
print(df[categorical_columns].head())

# Create LabelEncoder object: le
le = LabelEncoder()

# Apply LabelEncoder to categorical columns
df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))

# Print the head of the LabelEncoded categorical columns
print(df[categorical_columns].head())


--- Encoding categorical columns II: OneHotEncoder
# Import OneHotEncoder
from sklearn.preprocessing import OneHotEncoder

# Create OneHotEncoder: ohe
ohe = OneHotEncoder(categorical_features=categorical_mask, sparse=False)

# Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded
df_encoded = ohe.fit_transform(df)

# Print first 5 rows of the resulting dataset - again, this will no longer be a pandas dataframe
print(df_encoded[:5, :])

# Print the shape of the original DataFrame
print(df.shape)

# Print the shape of the transformed array
print(df_encoded.shape)



--- Encoding categorical columns III: DictVectorizer

# Import DictVectorizer
from sklearn.feature_extraction import DictVectorizer

# Convert df into a dictionary: df_dict
df_dict = df.to_dict("records")

# Create the DictVectorizer object: dv
dv = DictVectorizer(sparse=False)

# Apply dv on df: df_encoded
df_encoded = dv.fit_transform(df_dict)

# Print the resulting first five rows
print(df_encoded[:5,:])

# Print the vocabulary
print(dv.vocabulary_)





--- Preprocessing within a pipeline

# Import necessary modules
from sklearn.feature_extraction import DictVectorizer
from sklearn.pipeline import Pipeline

# Fill LotFrontage missing values with 0
X.LotFrontage = X.LotFrontage.fillna(0)

# Setup the pipeline steps: steps
steps = [("ohe_onestep", DictVectorizer(sparse=False)),
         ("xgb_model", xgb.XGBRegressor())]

# Create the pipeline: xgb_pipeline
xgb_pipeline = Pipeline(steps)

# Fit the pipeline
xgb_pipeline.fit(X.to_dict("records"), y)




---- Cross-validating your XGBoost model

# Import necessary modules
from sklearn.feature_extraction import DictVectorizer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score

# Fill LotFrontage missing values with 0
X.LotFrontage = X.LotFrontage.fillna(0)

# Setup the pipeline steps: steps
steps = [("ohe_onestep", DictVectorizer(sparse=False)),
         ("xgb_model", xgb.XGBRegressor(max_depth=2, objective="reg:linear"))]

# Create the pipeline: xgb_pipeline
xgb_pipeline = Pipeline(steps)

# Cross-validate the model
cross_val_scores = cross_val_score(xgb_pipeline, X.to_dict("records"), y, cv=10, scoring="neg_mean_squared_error")

# Print the 10-fold RMSE
print("10-fold RMSE: ", np.mean(np.sqrt(np.abs(cross_val_scores))))





---- Kidney disease case study I: Categorical Imputer
# Import necessary modules
from sklearn_pandas import DataFrameMapper
from sklearn_pandas import CategoricalImputer

# Check number of nulls in each feature column
nulls_per_column = X.isnull().sum()
print(nulls_per_column)

# Create a boolean mask for categorical columns
categorical_feature_mask = X.dtypes == object

# Get list of categorical column names
categorical_columns = X.columns[categorical_feature_mask].tolist()

# Get list of non-categorical column names
non_categorical_columns = X.columns[~categorical_feature_mask].tolist()

# Apply numeric imputer
numeric_imputation_mapper = DataFrameMapper(
                                            [([numeric_feature],Imputer(strategy="median")) for numeric_feature in non_categorical_columns],
                                            input_df=True,
                                            df_out=True
                                           )

# Apply categorical imputer
categorical_imputation_mapper = DataFrameMapper(
                                                [(category_feature, CategoricalImputer()) for category_feature in categorical_columns],
                                                input_df=True,
                                                df_out=True
                                               )
                                               
                                               
                                               
---- Kidney disease case study II: Feature Union   

# Import FeatureUnion
from sklearn.pipeline import FeatureUnion

# Combine the numeric and categorical transformations
numeric_categorical_union = FeatureUnion([
                                          ("num_mapper", numeric_imputation_mapper),
                                          ("cat_mapper", categorical_imputation_mapper)
                                         ])
                                         
                                         
------ Kidney disease case study III: Full pipeline
# Create full pipeline
pipeline = Pipeline([
                     ("featureunion", numeric_categorical_union),
                     ("dictifier", Dictifier()),
                     ("vectorizer", DictVectorizer(sort=False)),
                     ("clf", xgb.XGBClassifier(max_depth=3))
                    ])

# Perform cross-validation
cross_val_scores = cross_val_score(pipeline, kidney_data, y, scoring="roc_auc", cv=3)

# Print avg. AUC
print("3-fold AUC: ", np.mean(cross_val_scores))


------------------- Bringing it all together -------------------
# Create the parameter grid
gbm_param_grid = {
    'clf__learning_rate': np.arange(.05, 1, .05),
    'clf__max_depth': np.arange(3,10, 1),
    'clf__n_estimators': np.arange(50, 200, 50)
}

# Perform RandomizedSearchCV
randomized_roc_auc = RandomizedSearchCV(estimator=pipeline,
                                        param_distributions=gbm_param_grid,
                                        n_iter=2, scoring='roc_auc', cv=2, verbose=1)

# Fit the estimator
randomized_roc_auc.fit(X, y)

# Compute metrics
print(randomized_roc_auc.best_score_)
print(randomized_roc_auc.best_estimator_)






































































 
    
    
    
    
    
    
    
    
    
    
    
    

