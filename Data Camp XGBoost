RMSE root mean squared error  sqrt(sum(yi-y_pred)^2 / n)
MAE  mean absolute error     |(yi-y_pred)| / n



.iloc[:,:-1]  select all columns without last
.iloc[:,-1]  select the last column



#################################### Course 1: Classification with XGBoost  ####################################
- supervised learning
  AUC: metric for binary classification models
  (area under the ROC curve (x: false positive rate y: true positive rate)
  large AUC  = better model
  
  confusion matrix
  accuracy: tp + tn / (tp + tn + fp + fn)
  
  
  What is XGBoost/
  optimised  gradient-boostting machine learning library
  speed and performance
  core algorithm is parallelizable
  consistently outperforms single-algorithm methods
  
  
  
  import xgbosst as xgb
  import pandas as pd
  import numpy as np
  from  sklearn.model_selection import train_test_split
  
  class_data= pd.read_csv("xxxx")
  X,y=class_data.iloc[:,:-1],class_data.iloc[:,-1]
  X_train,y_train=
  xg_cl= xgb.XGBClassifier(objective ='binary:logistic',n_estimator= 10, seed = 123)
  xg_cl.fit(X_train,y_train)
  preds= xg_cl.predict(X_test)
  
  accuracy = float(np.sum(preds ==y_test))/y_test.shape[0]
  print("accuracy: %f" %(accuracy) )
  
  
  
XGBoost is usually used with trees as base learners
 ( tree is combinations of several binary questions)
individual tree tends to overfit


 

--------  What is Boosting?
At bottom, boosting isn't really a specific machine learning algorithm, but a concept that can be applied to a set of machine learning models. 
So, its really a meta-algorithm. 
Specifically, it is an ensemble meta-algorithm primarily used to reduce any given single learner's variance and to convert many weak learners into an 
arbitrarily strong learner.


Approaches:
 - iteratively learning a set of weak models on subsets of the data
 - weighing each weak prediction according to each weak learner's perfromance
 - combine all weighted predicted to obtain a single weighted prediction


Model evaluation through cross-validation
Cross validation is a robust method for estimating the performance of a model on unseen data

Example:

import xgboost as xgb
import pandas as pd
churn_data = pd.read_csv('xx')
churn_dmatrix = xgb.DMatrix(data=churn_data.iloc[:,:-1],label=churn_data.month_5_still_here)
params = {"objective":"binary:logistic","max_depth":4}
cv_results = xgb.cv(dtrain=churn_dmatrix, params = params, nfold =4, num_boost_round=10,metrics="error",as_pandas = True)
print("Accuracy:%f"%((1-cv_results["test-error-mean"]).iloc[-1]))

## 
Perform n-fold cross-validation by calling xgb.cv(). 
params is your parameter dictionary, 
nfold is the number of cross-validation folds , 
num_boost_round is the number of trees we want to build , 
metrics is the metric you want to compute (this will be "error", which we will convert to an accuracy).



-- When should we use XGBoost
- you have a large number of training samples
    a. Greater than 1000 training samples and less 100 features
    b. the number of features < number of training samples
- you have a mixture of categories and numeric features
    or just numeric features
    

NOT USE:
imagine recognition
computer vision
natural language processing and understanding problems
number of training samples < number of features








#################################### Course 2: Regression with XGBoost  ####################################

Objective (loss) functions and base learners


# Convert the training and testing sets into DMatrixes: DM_train, DM_test
DM_train = xgb.DMatrix(data=X_train, label=y_train)
DM_test =  xgb.DMatrix(data=X_test, label=y_test)

# Create the parameter dictionary: params
params = {"booster":"gblinear", "objective":"reg:linear"}

# Train the model: xg_reg
xg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=5)

# Predict the labels of the test set: preds
preds = xg_reg.predict(DM_test)

# Compute and print the RMSE
rmse = np.sqrt(mean_squared_error(y_test,preds))
print("RMSE: %f" % (rmse))






# Create the DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X,label=y)

# Create the parameter dictionary: params
params = {"objective":"reg:linear", "max_depth":4}



1. 
# Perform cross-validation: cv_results and "rmse"
cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics="rmse", as_pandas=True, seed=123)

# Print cv_results
print(cv_results)

# Extract and print final boosting round metric
print((cv_results["test-rmse-mean"]).tail(1))

2.
# Perform cross-validation: cv_results and  "mae"
cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics="mae", as_pandas=True, seed=123)

# Print cv_results
print(cv_results)

# Extract and print final round boosting round metric
print((cv_results["test-mae-mean"]).tail(1))





----- Regularization and base learners in XGBoost

regularization paramaters in XGBoost
 gamma
 alpha - l1 regularization
 lambda - l2 regularization
 
 
Base learners in XGBoost
linear base learner:
 sum of linear terms
 boosted model is weighted sum of linear models (thus is linear itself)
 rarely used
 
Tree base learner
  decision tree
  Boosted model is weighted sum of decision trees (nonlinear)
  Almost exclusively used in XGBoost
  
  
  
  Creating DataFrames from multiple equal-length lists
  
  pd.DataFrame(list(zip(list1,lists2)),columns=["list1","list2"])
   -- zip creates a generator of parallel values: zip([1,2,3],["a","b","c"]) = [1,"a"],[2,"b"],[3,"c"]
  
  
----- Using regularization in XGBoost  
  
  # Create the DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X, label=y)

reg_params = [1, 10, 100]

# Create the initial parameter dictionary for varying l2 strength: params
params = {"objective":"reg:linear","max_depth":3}

# Create an empty list for storing rmses as a function of l2 complexity
rmses_l2 = []

# Iterate over reg_params
for reg in reg_params:

    # Update l2 strength
    params["lambda"] = reg
    
    # Pass this updated param dictionary into cv
    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix , params=params, nfold=2, num_boost_round=5, metrics="rmse", as_pandas=True, seed=123)
    
    # Append best rmse (final round) to rmses_l2
    rmses_l2.append(cv_results_rmse["test-rmse-mean"].tail(1).values[0])

# Look at best rmse per l2 param
print("Best rmse as a function of l2:")
print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=["l2", "rmse"]))
  
  
  
  
  -------- Visualizing individual XGBoost trees
  
  # Create the DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X, label=y)

# Create the parameter dictionary: params
params = {"objective":"reg:linear", "max_depth":2}

# Train the model: xg_reg
xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)

# Plot the first tree
xgb.plot_tree(xg_reg, num_trees=0)
plt.show()

# Plot the fifth tree
xgb.plot_tree(xg_reg, num_trees=4)
plt.show()

# Plot the last tree sideways
# Plot the last (tenth) tree sideways. To do this, specify the additional keyword argument rankdir="LR"
xgb.plot_tree(xg_reg, num_trees=9, rankdir='LR')
plt.show()
  
  
  
-- Visualizing feature importances: What features are most important in my dataset

  # Create the DMatrix: housing_dmatrix
housing_dmatrix = xgb.DMatrix(data=X,label=y)

# Create the parameter dictionary: params
params = {"objective":"reg:linear","max_depth":4}

# Train the model: xg_reg
xg_reg = xgb.train(params=params,dtrain=housing_dmatrix,num_boost_round=10)

# Plot the feature importances
xgb.plot_importance(xg_reg)
plt.show()
  
  
  
  
  
  
  
########################### Course 3: Fine-tuning your XGBoost model ########################################
When is tuning your model a bad idea?
You are very short on time before you must push an initial model to production and have little data to train your model on.





  X,y = housing_data[housing_data.columns.tolist()["-1]], housing_data[housing_data.columns.tolist()[-1]]
   # ()[:-1] It slices the string to omit the last character, this works even on empty strings, it's a pretty safe way of removing that last character
  housing_dmatrix=xgb.DMatrix(data=X,label=y)
  untuned_params = {"objective":"reg:linear"}
  untuned_cv_results_rmse = xgb.cv(dtrain = housing_dmatrix,params=untuned_params,nfold=4,metrics="rmse",as_pandas=True,seed=123)
  print("Untuned rmse: %f" %(untuned_cv_results_rmse["test-rmse-mean"]).tail(1)))
  
  tuned_params = {"objective":"reg:linear",'colsample_bytree':0.3,'learning_rate':0.1,'max_depth':5}
  tuned_cv_results_rmse = xgb.cv(dtrain = housing_dmatrix,params=tuned_params,nfold=4,num_boost_round=200,metrics="rmse",as_pandas=True,seed=123)
  print("Untuned rmse: %f" %(tuned_cv_results_rmse["test-rmse-mean"]).tail(1)))


  
