############################# Course 1: Applying logistic regression and SVM  #############################

- fitting and predicting

import sklearn.datasets

newsgroups = sklearn.datasets.fetch_20newsgroups_vectorized()

X, y = newsgroups.data, newsgroups.target


-- Fitting and Predicting

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors = 1 )

knn.fit(X,y)
y_pred = knn.predict(X)

-- Model evaluation

knn.score(X,y)

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X,y,testsize=0.x)

################################################################################################################################################################

-----   KNN
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors = 1 )
knn.fit(X_train,y_train)
knn.predict(X_test)
knn.score(X_test,y_test)



----- Applying logistic regression and SVM

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(X_train,y_train)
lr.predict(X_test)
lr.score(X_test,y_test)


----- Using  SVC C-Support Vector Classification.
from sklearn.svm import SVC
svm = SVC()



----- Visualizing decision boundaries

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC, LinearSVC
from sklearn.neighbors import KNeighborsClassifier

# Define the classifiers
classifiers = [LogisticRegression(),LinearSVC(),SVC(),KNeighborsClassifier()] ---> Never forget the ()

# Fit the classifiers
for c in classifiers:
    c.fit(X,y)

# Plot the classifiers
plot_4_classifiers(X, y, classifiers)
plt.show()



############################################ Course 2: Loss function ##########################################
x@y is called the dot product of x and y, and is written x . y

lr.coef_@X[10] + lr.inttercept_ #  raw model

Which classifiers make predictions based on the sign (positive or negative) of the raw model output?
Both Logistic regression and Linear SVM


What is a loss function?
linear regression minimise a loss: the least square sum(real-predicted)^2
model.score() is not necessarly for loss function

classficaion errors: he 0-1 loss
0 for correct, 1 for incorrect prediction
this loss is hard to minimise

--- 
from scipy.optimize import minimize
minimize(np.square,0).x
--> array([0.]) minmised when x = 0

minimize(np.square,2).x
-> array([-1.88848e_08])


# Mathematical functions for logistic and hinge losses
def log_loss(raw_model_output):
   return np.log(1+np.exp(-raw_model_output))
def hinge_loss(raw_model_output):
   return np.maximum(0,1-raw_model_output)
   
# Create a grid of values and plot
grid = np.linspace(-2,2,1000)
plt.plot(grid, log_loss(grid), label='logistic')
plt.plot(grid, hinge_loss(grid), label='hinge')
plt.legend()
plt.show()


# The logistic loss, summed over training examples
def my_loss(w):
    s = 0
    for i in range(y.size):
        raw_model_output = w@X[i]
        s = s + log_loss(raw_model_output * y[i])
    return s

# Returns the w that makes my_loss(w) smallest
w_fit = minimize(my_loss, X[0]).x
print(w_fit)

# Compare with scikit-learn's LogisticRegression
lr = LogisticRegression(fit_intercept=False, C=1000000).fit(X,y)
print(lr.coef_)



##################### Course 3: 3  Logistic regression  ####################

In scikit-learn, the hyperparameter "C" is the inverse of the regularization strength. 
In other words, larger C means less regularization and smaller C means more regularization. Let's test this out.

 
 
 How does regularization affect test accuracy?

lr_weak_reg = LogisticRegression(C=100)
lr_strong_reg = LogisticRegressiioin(C=0.01)

lr_weak_reg.fit(X_train,y_train)
lr_strong_reg.fit(X_train,y_train)

lr_weak_score(X_train,y_train)
lr_strong_score(X_train,y_train)

==> regularized loss = original loss + large coefficient penalty
more regularization: lower training accuaracy
more regularization: (almost always) higher test accuaracy



L1 vs L2 regularizaion
Lasso = linear regression with L1 regularization
Ridge = linear regression with L2 regularization

lr_L1= LogisticRegression(penalty ='l1')
lr_L2 =LogisticRegression() # penalty='l2' by default

lr_L1.fit(X_train,y_train)
lr_L2.fit(X_train,y_train)

plt.plot(lr_L1.coef_.flatten())
plt.plot(lr_L2.coef_.flatten())



# Train and validaton errors initialized as empty list
train_errs = list()
valid_errs = list()

# Loop over values of C_value
for C_value in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:
    # Create LogisticRegression object and fit
    lr = LogisticRegression(C=C_value) -->  (C = value)
    lr.fit(X_train, y_train)
    
    # Evaluate error rates and append to lists
    train_errs.append( 1.0 - lr.score(X_train, y_train) )
    valid_errs.append( 1.0 - lr.score(X_valid, y_valid) )
    
# Plot results
plt.semilogx(C_values, train_errs, C_values, valid_errs)
plt.legend(("train", "validation"))
plt.show()



# Specify L1 regularization
lr = LogisticRegression(penalty='l1')

# Instantiate the GridSearchCV object and run the search
searcher = GridSearchCV(lr, {'C':[0.001, 0.01, 0.1, 1, 10]})
searcher.fit(X_train, y_train)

# Report the best parameters
print("Best CV params", searcher.best_params_)

# Find the number of nonzero coefficients (selected features)
best_lr = searcher.best_estimator_
coefs = best_lr.coef_
print("Total number of features:", coefs.size)
print("Number of selected features:", np.count_nonzero(coefs))




# Get the indices of the sorted cofficients
inds_ascending = np.argsort(lr.coef_.flatten()) 
inds_descending = inds_ascending[::-1]

# Print the most positive words
print("Most positive words: ", end="")
for i in range(5):
    print(vocab[inds_descending[i]], end=", ")
print("\n")

# Print most negative words
print("Most negative words: ", end="")
for i in range(5):
    print(vocab[inds_ascending[i]], end=", ")
print("\n")



---- Logistic regression and probabilities

logistic regression predictions: sign of raw model output
logistic regression probabilities: 'squashed raw model output

-- In this exercise, you will observe the effects of changing the regularization strength on the predicted probabilities.
-- different strengths: different C values, eg, C = 1, C = 0.1, C = 10

# Set the regularization strength
model = LogisticRegression(C=0.1)

# Fit and plot
model.fit(X,y)
plot_classifier(X,y,model,proba=True)

# Predict probabilities on training points
prob = model.predict_proba(X)
print("Maximum predicted probability", np.max(prob)) --> maximum np.max()


-- In this exercise, you'll visualize the examples that the logistic regression model 
is most and least confident about by looking at the largest and smallest predicted probabilities.

lr = LogisticRegression()
lr.fit(X,y)

# Get predicted probabilities
proba = lr.predict_proba(X)

# Sort the example indices by their maximum probability
proba_inds = np.argsort(np.max(proba,axis=1))

# Show the most confident (least ambiguous) digit
show_digit(proba_inds[-1], lr)

# Show the least confident (most ambiguous) digit
show_digit(proba_inds[0], lr)





--- Multi-class logistic regression
Multi-class classification means having more than 2 classes.


-- One-vs-rest vs. multinomial/softmax
lr0.fit(X,y==0)
lr1.fit(X,y==1)
lr2.fit(X,y==2)
# get raw model output
lr0.decision_function(X)[0]
lr1.decision_function(X)[0]
lr2.decision_function(X)[0]


-- Model coefficient for multi-class
# one -vs - rest by default
lr_ovr = LogisticRegression()
lr_ovr.fit(X,y)
lr_ovr.coef_.shape
lr_ovr.intercept_.shape

lr_mn = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs')
lr_mn.fit(X,y)
lr_mn.coef_.shape
lr_mn.intercept_.shape




# We'll use SVC instead of LinearSVC from now on
from sklearn.svm import SVC

# Create/plot the binary classifier (class 1 vs. rest)
svm_class_1 = SVC()
svm_class_1.fit(X_train, y_train==1)
plot_classifier(X_train, y_train==1, svm_class_1)






############################ Course 4: Support Vector Machines   ############################

# Train a linear SVM
svm = SVC(kernel="linear")
svm.fit(X,y)
plot_classifier(X, y, svm, lims=(11,15,0,6))

# Make a new data set keeping only the support vectors
print("Number of original examples", len(X))
print("Number of support vectors", len(svm.support_))
X_small = X[svm.support_]
y_small = y[svm.support_]

# Train a new SVM using only the support vectors
svm_small = SVC(kernel="linear")
svm_small.fit(X_small, y_small)
plot_classifier(X_small, y_small, svm_small, lims=(11,15,0,6))




----- Kernel SVMs

from sklearn.svm import SVC
svm=SVC(gamma=1) # default is kernal = 'rbf'
---> larger gamma leads to more complex boundaries

# Instantiate an RBF SVM
svm = SVC()

# Instantiate the GridSearchCV object and run the search
parameters = {'gamma':[0.00001, 0.0001, 0.001, 0.01, 0.1]}
searcher = GridSearchCV(svm, parameters)
searcher.fit(X, y)

# Report the best parameters and the corresponding score
print("Best CV params", searcher.best_params_)
print("Best CV accuracy", searcher.best_score_)

# Report the test accuracy using these best parameters
print("Test accuracy of best grid search hypers:", searcher.score(X_test, y_test))







------ Comparing logistic regression and SVM (and beyond)

# We set random_state=0 for reproducibility 
linear_classifier = SGDClassifier(random_state=0)

# Instantiate the GridSearchCV object and run the search
parameters = {'alpha':[0.00001, 0.0001, 0.001, 0.01, 0.1, 1], 
             'loss':['hinge', 'log'], 'penalty':['l1','l2']}
searcher = GridSearchCV(linear_classifier, parameters, cv=10)
searcher.fit(X_train, y_train)

# Report the best parameters and the corresponding score
print("Best CV params", searcher.best_params_)
print("Best CV accuracy", searcher.best_score_)
print("Test accuracy of best grid search hypers:", searcher.score(X_test, y_test))











































