#################################### Course 1: Importing data from the Internet ####################################
URL 

the urllib package -> url library
urlopen() - accepts URLs instead of file names

from urllib import urlretrieve
url = 'http://xxxxxx/xxx.csv'
urlretrieve(url,'xxxx.csv')---> urlretrieve : performs a GET request

----------------------------------
# Import package
from urllib.request import urlretrieve

# Import pandas
import pandas as pd

# Assign url of file: url
url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'

# Save file locally
urlretrieve(url,'winequality-red.csv')

# Read file into a DataFrame and print its head
df = pd.read_csv('winequality-red.csv', sep=';')
print(df.head())

----------------------------------
# Import packages
import matplotlib.pyplot as plt
import pandas as pd

# Assign url of file: url
url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'

# Read file into a DataFrame: df
df = pd.read_csv(url, sep=';')

# Print the head of the DataFrame
print(df.head())

# Plot first column of df
pd.DataFrame.hist(df.ix[:, 0:1])
plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')
plt.ylabel('count')
plt.show()



# Import package
import pandas as pd

# Assign url of file: url
url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'

# Read in all sheets of Excel file: xls
xls = pd.read_excel(url,sheet_name=None) --> Read the file in url into a dictionary xls using pd.read_excel()

# Print the sheetnames to the shell
print(xls.keys()) --> Print the names of the sheets in the Excel spreadsheet; these will be the keys of the dictionary xls

# Print the head of the first sheet (using its name, NOT its index)
print(xls['1700'].head()) --> Print the head of the first sheet using the sheet name, not the index of the sheet! The sheet name is '1700'


------------------------------------------------------------------------------------------------------
HTTP requests to import files from the web
( HyperText Transfer Protocol )

urlretrieve : performs a GET request

HTML
( HyperText Markup Language )



from urllib.request import urlopen,Request
url ='https:''www.xxxx.org/"
request = Request(url)
response = urlopen(request)
html=response.read()
response.close()

--> Get requests using requests
import requests
url = 'https://www.xxxx.org/"
r = requests.get(url)
text = r.text


------------------------------------------------------------------------------------------------------
Scraping the web in Python

Use the method find_all() to find all hyperlinks


# Import packages
import requests
from bs4 import BeautifulSoup

# Specify url: url
url = 'https://www.python.org/~guido/'

# Package the request, send the request and catch the response: r
r = requests.get(url)

# Extracts the response as html: html_doc
html_doc = r.text ---> Use the text attribute of the object r to return the HTML of the webpage as a string; 

# Create a BeautifulSoup object from the HTML: soup
soup = BeautifulSoup(html_doc)

# Prettify the BeautifulSoup object: pretty_soup
pretty_soup=soup.prettify() 

# Get the title of Guido's webpage: guido_title
guido_title = soup.title --> No () !!! --> xxx.title

# Print the title of Guido's webpage to the shell
print(guido_title)

# Get Guido's text: guido_text
guido_text = soup.get_text() --> get text: xxx.get_text() with ()

# Print Guido's text to the shell
print(guido_text)

# Find all 'a' tags (which define hyperlinks): a_tags
a_tags = soup.find_all('a')

# Print the URLs to the shell
for link in a_tags:
    print(link.get('href'))
--- The variable a_tags is a results set: your job now is to enumerate over it, using a for loop and to print the actual URLs of the hyperlinks; 
----- to do this, for every element link in a_tags, you want to print() link.get('href')

#################################### Course 2: Interacting with APIs to import data from the web ####################################
APIs
Application Programmin Interface

JSONs
JavaScript Object Notation, human readable

import json
with open('xxx.json','r') as json_file:
     json_data = json.load(json_file) -> as a dictionary
     
for key, value in json_data.items():
    print(key + ':', value)



# Load JSON: json_data
with open("a_movie.json") as json_file:
    json_data = json.load(json_file)

# Print each key-value pair in json_data
for k in json_data.keys():
    print(k + ': ', json_data[k])
    
    

APIs and interactin with the world wide web

--- connetcting to an API in Python
import requests
url ='http://xxx.com/xxx'
r = request.get(url)
json_data = r.json()
for key, value in json_data.itmes():
    print(key + ':',value)




# Import requests package
import requests

# Assign URL to variable: url
---> Assign to the variable url the URL of interest in order to query 'http://www.omdbapi.com' for the data corresponding 
---> to the movie The Social Network. The query string should have two arguments: apikey=72bc447a and t=the+social+network. 
---> You can combine them as follows: apikey=72bc447a&t=the+social+network

url = 'http://www.omdbapi.com/?apikey=72bc447a&t=the+social+network' --> /? &

# Package the request, send the request and catch the response: r
r = requests.get(url)

# Decode the JSON data into a dictionary: json_data
json_data = r.json()

# Print the text of the response
print(r.text) --> Print the text of the response object r by using its text attribute

# Print the Wikipedia page extract
pizza_extract = json_data['query']['pages']['24768']['extract']
print(pizza_extract)

#################################### Course 3: Diving deep into the Twitter API ####################################

The Twitter API and Authentication

Using Tweepy: authentication handler -> The package tweepy is great at handling all the Twitter API
tw_auth.py
import tweepy, json


Tweepy: define stream listener class
st_class.py
class MyStreamListener(tweepy.StreamListener):
      def  :
      
      def :

Using Tweepy: stream twests!!
tweets.py
# create streaming object and authenticate
l = MyStreamListener()
stream = tweepy.Stream(auth,l)
# This line filters Twitter Streams to capture data by keywords:
stream.filter(track =['apples','organes'])


---------- exercise

# Import package
import tweepy

# Store OAuth authentication credentials in relevant variables
access_token = "1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy"
access_token_secret = "X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx"
consumer_key = "nZ6EA0FxZ293SxGNg8g8aP0HM"
consumer_secret = "fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i"

# Pass OAuth details to tweepy's OAuth handler
# Pass the parameters consumer_key and consumer_secret to the function tweepy.OAuthHandler().
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
# Complete the passing of OAuth credentials to the OAuth handler auth by applying to it the method set_access_token(), 
# along with arguments access_token and access_token_secret.
auth.set_access_token(access_token, access_token_secret)

# Initialize Stream listener
l = MyStreamListener()

# Create your Stream object with authentication
stream = tweepy.Stream(auth, l)

# Filter Twitter Streams to capture data by the keywords:
stream.filter(track=['clinton','trump','sanders','cruz'])






# Import package
import json

# String of path to file: tweets_data_path
tweets_data_path = 'tweets.txt'

# Initialize empty list to store tweets: tweets_data
tweets_data = []

# Open connection to file
tweets_file = open(tweets_data_path, "r")

# Read in tweets and store in list: tweets_data
for line in tweets_file:
    tweet = json.loads(line)
    tweets_data.append(tweet)

# Close connection to file
tweets_file.close()

# Print the keys of the first tweet dict
print(tweets_data[0].keys())




-- Twitter data to DataFrame
# Import package
import pandas as pd

# Build DataFrame of tweet texts and languages
df = pd.DataFrame(tweets_data, columns=['text', 'lang'])

# Print head of DataFrame
print(df.head())


--- A little bit of Twitter text analysis
# Initialize list to store tweet counts
[clinton, trump, sanders, cruz] = [0, 0, 0, 0]

# Iterate through df, counting the number of tweets in which
# each candidate is mentioned
for index, row in df.iterrows():
    clinton += word_in_text('clinton', row['text'])
    trump += word_in_text('trump', row['text'])
    sanders += word_in_text('sanders', row['text'])
    cruz += word_in_text('cruz', row['text'])


--- Plotting your Twitter data

# Import packages
import matplotlib.pyplot as plt
import seaborn as sns

# Set seaborn style
sns.set(color_codes=True)

# Create a list of labels:cd
cd = ['clinton', 'trump', 'sanders', 'cruz']

# Plot the bar chart
ax = sns.barplot(cd, [clinton, trump, sanders, cruz])
ax.set(ylabel="count")
plt.show()





----------------------- Practice 
df = pd.DataFrame(tweets_data,columns = ['lang','text'])print(df.head(2)) # no " " for database

A set of protocols and routines for building and interacting with software applications: API

