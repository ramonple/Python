##################################  Course 1: Read, clean and validate  ###################################################
value_counts().sort_index()
.sort_values()
-------------------------
shape : To get the number of rows and columns in a DataFrame, you can read its shape attribute.
data.shape

columns: To get the column names
data.columns

# Select the column 'birthwgt_oz1' and assign it to a new variable called ounces
ounces = data['birthwgt_oz1']
# Print the first 5 elements of ounces
print(ounces.head())

clean and validate

pounds.value_counts().sort_index()
pounds.decribe()
ounces.replace([98, 99], np.nan, inplace=True) # replaced the values 98 and 99 in the ounces column using the .replace() method:
pounds.mean()

--exercise
# Replace the value 8 with NaN
nsfg['nbrnaliv'].replace([8], np.nan, inplace=True)

# Print the values and their frequencies
print(nsfg['nbrnaliv'].value_counts())


arithmetic with series
birth_weight = pounds + pounds / 16






Filter and visualise

import matplotlib.pyplot as plt

plt.hist(birth_weight.dropna(), bins=30)
plt.xlabel()
plt.ylabel()



Boolean Series
preterm = nsfg['prglngth']<37
preterm.sum() --> For each, 1 for True and 0 for False
preterm.mean()

Filtering based on Boolean Series
preterm_weight = birth_weight[preterm]

logical operators: & , | (or)


some groups are 'oversampled':
resample_rows_weighted()




-- exercise:
# Create a Boolean Series for full-term babies
full_term = nsfg['prglngth'] >= 37

# Select the weights of full-term babies
full_term_weight = birth_weight[full_term] -- NO ''

# Compute the mean weight of full-term babies
print(full_term_weight.mean())



# Filter full-term babies
full_term = nsfg['prglngth'] >= 37

# Filter single births
single = nsfg['nbrnaliv'] == 1

# Compute birth weight for single full-term babies
single_full_term_weight = birth_weight[single & full_term]. --> how to set one than one filters
print('Single full-term mean:', single_full_term_weight.mean())

# Compute birth weight for multiple full-term babies
mult_full_term_weight = birth_weight[~single & full_term]
print('Multiple full-term mean:', mult_full_term_weight.mean())



##################################################### Couse 2: Distributions ######################################################################################
Pmf(data,normalize=True/False)
Pmf(data['column'],normalize=True/False)
Pmf(data['column'].plot()
Pmf(data['column'].bar()

Cdf(data['column']).plot()
Cdf(data['column']).inverse()

from scipy.stats import norm
xs=np.linspace(a,b)
ys=norm(0,1).cdf(xs)
plt.plot(xs,ys,color='gray')

ys = dist.cdf(xs)

log_income = np.log10(income) 

sns.kdeplot(log_income)

-------------------------------------

PMF: probability mass function

pmf_educ = Pmf(educ,normalize=False)
pmf_edc[12] --> 47291

pmf_educ = Pmf(educ,normalize=True)
pmf_edc[12] --> 0.308294

pmf_educ.bar(label='educ')


pmf_year = Pmf(gss['year'], normalize=False)

# Print the result
print(pmf_year)

plt.plot(xs, ys, color='gray')

------------------------------
Cumulative distriburion functions

cdf=Cdf(gss['age'])
cdf.plot()
plt.xlabel('age')
plt.ylabel('CDF')
plt.show()

# evaluating the CDF
q=51
p=cdf(q)
print(p) --> 0.66


# inverse CDF
 p =0.25
 q = cdf.inverse(p)
 print(q) --> 30
 

--- exercise:
cdf_age(30) ---> column: age = gss['age']

-----------
# Calculate the 75th percentile 
percentile_75th = cdf_income.inverse(0.75)

# Calculate the 25th percentile
percentile_25th = cdf_income.inverse(0.25)

# Calculate the interquartile range
iqr = percentile_75th - percentile_25th
# Print the interquartile range
print(iqr)





Comparing distributions

-- multiple PMFs
male = gss['sex']==1
age=gss['age']
male_age=age[male]
female_age=age[~male]
Pmf(male_age).plot(label='Male')
Pmf(female_age).plot(label='Female')


-- multiple CDFs
Cdf(male_age).plot(label='Male')
Cdf(female_age).plot(label='Female)

income=gss['realinc']
pre95=gss['year'] < 1995
Pmf(income[pre95]).plot(label='Before 1995')
Pmf(income[~pre95).plot(label='After 1995')


--- exercise
# Select educ
educ = gss['educ']

# Bachelor's degree
bach = (educ >= 16)

# Associate degree
assc = (educ >= 14) & (educ < 16)

# High school
high = (educ <= 12)
print(high.mean())




Modeling distributions

--- Normal distribution:
sample = np.random.normal(size=1000)
Cdf(sample).plot()

--- The normal CDF
from scipy.stats import norm
xs=np.linspace(-3,3)
ys=norm(0,1).cdf(xs)
plt.plot(xs,ys,color='gray')

xs=np.linspace(-3,3)
ys=norm(0,1).pdf(xs)
plt.plot(xs,ys,color='gray')


KDE plot # kernel density estimate
import seaborn as sns
sns.kdeplot(sample)


-- exercise
# Extract realinc and compute its log
income = gss['realinc']
log_income = np.log10(income) --> calculate the log value

# Compute mean and standard deviation
mean = log_income.mean()
std =log_income.std()
print(mean, std)

# Make a norm object 
# Make a norm object by passing the computed mean and standard deviation to norm()
from scipy.stats import norm
dist = norm(mean,std)

# Evaluate the model CDF
xs = np.linspace(2, 5.5)
ys = dist.cdf(xs) ---> calculate the cdf of the distribution

# Plot the model CDF
plt.clf()
plt.plot(xs, ys, color='gray')

# Create and plot the Cdf of log_income
Cdf(log_income).plot()
    
# Label the axes
plt.xlabel('log10 of realinc')
plt.ylabel('CDF')
plt.show()



# Evaluate the normal PDF
xs = np.linspace(2, 5.5)
ys = dist.pdf(xs)

# Plot the model PDF
plt.clf()
plt.plot(xs, ys, color='gray')

# Plot the data KDE
sns.kdeplot(log_income)
    
# Label the axes
plt.xlabel('log10 of realinc')
plt.ylabel('PDF')
plt.show()


##################################################### Course 3: Relationships #####################################################
---- Exploring relationships
-> scatter plot
plt.plot()

transparency (alpha=)
marker size (markersize)
plt.plot(x,y,'o',markersize = 1, alpha=0.02)


Jittering ---> adding random noise
height_jitter = height + np.random.normal(0,2,size=len(brfss))
plt.plot(height_jitter,weight,'0',markersize=1,alpha=0.02)


Zoom
plt.plot(height_jitter,weight_jitter,'o',markersize=1,alpha=0.02)
plt.axis([140,200,0,160]) --> upper and lower bounds for x and y

--- exercise

# Select the first 1000 respondents
brfss = brfss[:1000]





Visualizing relationships

-> Violin plot
data = brfss.dropna(subset=['AGE','WTKG3'])
sns.violinplot(x='AGE',y='WTKG3',data=data, inner=None) --> Never forget x= y=, data = xxx

-> Boxplot
sns.boxplot(x,y,data,whis=10)
-- logscale
plt.yscale('log')







Correlation
# Select columns
columns = ['AGE', 'INCOME2', '_VEGESU1']
subset = brfss[columns]

# Compute the correlation matrix
print(subset.corr())





simple regression
from scipy.stats import linreagress
res=linregress(xs,ys)


Regression lines
fx=np.array([xs.min(),xs.max()]) --> never forget the [ ]
                                 ---> Set fx to the minimum and maximum of xs, stored in a NumPy array.
fy=res.intercep + res.slope * fx
plt.plot(fx,fy,'-')

subset = btfss.dropna(subset=['a','b'])
xs=subset['a']
ys=subset['b']
res = linregres(xs,ys)




##################################################### Course 4: Multivariate Thinking #####################################################
-------- Limits of simple regression
--> multiple regression

import statsmodels.formula.api as smf
results = smf.los('income2~_vengesu1',data=brfss).fit()
results.params

--> exercise:
from scipy.stats import linregress
import statsmodels.formula.api as smf

# Run regression with linregress
subset = brfss.dropna(subset=['INCOME2', '_VEGESU1'])
xs = subset['INCOME2']
ys = subset['_VEGESU1']
res = linregress(xs,ys)
print(res)

# Run regression with StatsModels
results = smf.ols('_VEGESU1 ~ INCOME2', data = brfss).fit()
print(results.params)



---------- Multiple regression
gss=pd.read_hdf('gss.hdf5','gss')
results = smf.los('realinc~educ',data=gss).fit() # realinc is the one we want to predict, educ is the one we use to form the prediction
results.params

# Adding age
results = smf.los('realinc~educ + age',data=gss).fit()

grouped = gss.groupby('age') --> Notice!! is () not []
mean_income_by_age = grouped['realinc'].mean()

# adding a quadratic term
gss['age2']= gss['age'] ** 2
model=smf.ols('realinc ~ educ + age + age2',data=gss) --> notice () not []
results = model.fit() --> .fit()
# Print the estimated parameters
results.params



----- Visualizing regression results

-- Generating predictions
df = pd.DataFrame()
df['age]=np.linspace(18,58)
df['age2']= df['age']**2

df['educ']=12
df['educ2']=df['educ']**2

pred12=results.predict(df)

-- plotting prediction
plt.plot(df['age'],pred12,label='High school']
plt.plot(mean_income_by_age,'o',alpha=0.5)

--> exercise

# Run a regression model with educ, educ2, age, and age2
results = smf.ols('realinc ~ educ + educ2 + age + age2', data=gss).fit()

# Make the DataFrame
df = pd.DataFrame()
df['educ'] = np.linspace(0,20)
df['age'] = 30
df['educ2'] = df['educ']**2
df['age2'] = df['age']**2

# Generate and plot the predictions
pred = results.predict(df)
print(pred.head())

# Plot mean income in each age group
plt.clf() --> clears the entire current figure with all its axes, but leaves the window opened,
grouped = gss.groupby('educ')
mean_income_by_educ = grouped['realinc'].mean()
plt.plot(mean_income_by_educ,'o',alpha=0.5)



--- Logistic regression
-- categorical variables
formula = 'realinc ~ educ + educ2 + age + age2 + C(sex)' --> sex is the categorical variable
results = smf.ols(formula,data=gss).fit()
result.params

-- Boolean variable
gss['gunlaw'].value_counts()
-> output: 1.0 30245, 2.0 8934
gss['gunlaw'].replace([2],[0],inplace=True)
gss['gunlaw'].value_counts()
--> output: 1.0 30245, 0.0 8934


-- logistic regression
formula = 'realinc ~ educ + educ2 + age + age2 + C(sex)' --> sex is the categorical variable
results = smf.logit(formula,data=gss).fit() --> from 'ols' to 'logit'
result.params

-- generating predictions
df = pd.DataFrame()
df['age']=np.linspace(18,89)
df['educ']=12
df['age2']=df['age']**2
df['educ2']=df['educ'] ** 2
df['sex']=1
pred1 = results.predict(df)
df['sex']=2
pred2=results.predic(df)
-- visualise the results
grouped = gss.groupby('age')
favor_by_age = grouped['gunlaw'].mean()
plt.plot(favor_by_age,'o',alpha=0.5)
plt.plot(df['age'],pred1,label='Male')
plt.plot(df['age'],pred2,label='Female')


-- exercise:
# Recode grass
gss['grass'].replace(2, 0, inplace=True)

# Run logistic regression
results = smf.logit('grass ~ age + age2 + educ + educ2 + C(sex)', data=gss).fit()
results.params

# Make a DataFrame with a range of ages
df = pd.DataFrame()
df['age'] = np.linspace(18, 89)
df['age2'] = df['age']**2

# Set the education level to 12
df['educ'] = 12
df['educ2'] = df['educ']**2

# Generate predictions for men and women
df['sex'] = 1
pred1 = results.predict(df)

df['sex'] = 2
pred2 = results.predict(df)

plt.clf()
grouped = gss.groupby('age')
favor_by_age = grouped['grass'].mean()
plt.plot(favor_by_age, 'o', alpha=0.5)

plt.plot(df['age'], pred1, label='Male')
plt.plot(df['age'], pred2, label='Female')

plt.xlabel('Age')
plt.ylabel('Probability of favoring legalization')
plt.legend()
plt.show()

