#################################  Data Cleaning  ##################################################################

# Strip duration of minutes
ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip("minutes")

# Convert duration to integer
ride_sharing['duration_time'] = ride_sharing["duration_trim"].astype(int)

# Write an assert statement making sure of conversion
assert ride_sharing['duration_time'].dtype == 'int'

# Print formed columns and calculate average ride duration
print(ride_sharing[['duration','duration_trim','duration_time']])
print((ride_sharing['duration_time']).mean())



# table value constraints

Use .loc[] to set all values of xx in a range

# Convert tire_sizes to integer ->xxx.astype(' ')
ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')

example" Set all values above 27 to 27 -- .loc[xxx>xx,xxx] = yyy
ride_sharing.loc[ride_sharing["tire_sizes" >27, "tire_sizes"] = 27


# Convert ride_date to datetime
ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date'])

# Save today's date
today = dt.date.today()

# Set all in the future to today's date
ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today


### uniqueness constraints -> duplicated values
- find the duplicated values: .duplicated()
.duplicated(subset = , keep=)
subset: list of column names to check for duplication
keep: whether to keep first('first'), last('last') or all(False) duplicated values


duplicates = df.duplicated()
print(duplicates) -> will print true or false
df[duplicates] -> output the duplicated information
dr[duplicates].sort_values(by ='xxx')

How to treat duplicate values?
.drop_duplicates()
inplace: drop duplicated rows directly inside DateFrame without creating new objects, inplace = True

.groupby()
.agg()



### Membership constraints

> finding inconsistent categories
inconsistent_categories = set(study_data['blood_type']).difference(categories['blood_type'])
print(inconsistent_categories)

> find the incosistent rows
inconsistent_rows = study_date['blood_type'].isin(inconsistent_catogies)

> drop inconsistent rows
inconsistent_data = study_data[incosistent_rows] - find
# Drop
inconsistent_data = study_data[~incosistent_rows]

## Categorical variables
a. value consistency
Upper and Lower case:
1. marriage_status['marriage_status'] = marriage_status['marriage_status'].str.upper()
   marriage_status['marriage_status'] = marriage_status['marriage_status'].str.lower()
b. blank space
   marriage_status['marriage_status'] = marriage_status['marriage_status'].str.strip()
c. collapsing data into categories
  1) >>> create categories out of data: income_group column from income column
    import pandas as pd
    ranges=[0,200000,500000,np.inf]
    group_names = ['0-200k','200k-500k','500k+']
    demographics['income_group']=pd.cut(demographics['household_income'],bins=ranges,labels = group_names)
    demographics[['income_group'],['household_income']]
 
 example:
 # Create ranges for categories
label_ranges = [0, 60, 180, np.inf]
label_names = ['short', 'medium', 'long']

# Create wait_type column
airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, labels = label_names)

# Create mappings and replace
mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', 
            'Thursday': 'weekday', 'Friday': 'weekday', 
            'Saturday': 'weekend', 'Sunday': 'weekend'}

airlines['day_week'] = airlines['day'].replace(mappings)

  
  2) >>> map categories to fewer ones
  operation_system column is 'a','b','c','d','e'
  operation_system column should be ' abc','de'
  
  mapping ={a':'abc','b':'abc,'c':'abc,'d':'de','e':'de'}
  devices['operation_system']=devices['operating_system'].replace(mapping)
  devices['operation_system'].unique()
  
  d. replace
  airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})
    



>>>>>>>>>>>  Cleaning text data
# replace '+" with '00'
phones["phone number"] = phones["phones number"].str.replace("+","00")
# replace phone number with lower than 10 digits to Nan
digits = phones["phone number"] .str.len()
phones.loc[digicts < 10, "phone number"] = np.nan

-- example:
# Store length of each row in survey_response column
resp_length = airlines['survey_response'].str.len()

# Find rows in airlines where resp_length > 40
airlines_survey = airlines[resp_length > 40]

# Assert minimum survey_response length is > 40
assert airlines_survey['survey_response'].str.len().min() > 40

# Print new survey_response column
print(airlines_survey['survey_response'])


# regular expresion
# replace letters with nothing
phones["phone number"] = phones["phones number"].str.replae(r'\D+','')
--  r'(\d+)' This is a Regular Expression pattern \d is a regex pattern for digit
-- + is a regex pattern for at least (one or more) since they are enclosed in a ( ) that means the group that you want to capture



## Uniformity
pandas.to_datetime()
birthday["Birthay'] = birthday['Birthday'].dt.strftime('%d-%m-%Y')

# Find values of acct_cur that are equal to 'euro'
acct_eu = banking['acct_cur'] == 'euro'

# Convert acct_amount where it is in euro to dollars
banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1 

# Unify acct_cur column by changing 'euro' values to 'dollar'
banking.loc[acct_eu, 'acct_cur'] = 'dollar'

# Assert that only dollar currency remains
assert banking['acct_cur'].unique() == 'dollar'



# Print the header of account_opend
print(banking['account_opened'].head())

# Convert account_opened to datetime
banking['account_opened'] = pd.to_datetime(banking['account_opened'],
                                           # Infer datetime format
                                           infer_datetime_format = True,
                                           # Return missing value for error
                                           errors = 'coerce')  

# Get year of account opened
banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')

# Print acct_year
print(banking['acct_year'])


