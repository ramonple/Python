########################################## Course 1: Common data problems ##########################################
----- Data type

# get the data types of columns
sales.dtypes

# print sum of all Revenue column
sales['Revenue'].sum()

String to integers: --> str.strip('')
# Remove $ from Revenue column
sales['Revenue'] = sales['Revenue'].str.strip('$') --> .str.strip('xx')
sales['Revenue'] = sales['Revenue'].astype('int')

# verify that Revenue is now an integer
assert sales['Revenue'].dtype == 'int'


Numeric or categorical?
# convert to categorical:
df['marriage_status'] = df['marriage_status'].astype('category')

# Print the information of ride_sharing
print(ride_sharing.info()) ---> data.info()

# Print summary statistics of user_type column
print(ride_sharing['user_type'].describe()) --> data.decribe()

# Convert user_type from integer to category
ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category)

# Write an assert statement confirming the change
assert ride_sharing['user_type_cat'].dtype == 'category'

# Print new summary statistics 
print(ride_sharing['user_type_cat'].describe())

# Strip duration of minutes
ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip("minutes")

# Print formed columns and calculate average ride duration 
print(ride_sharing[['duration','duration_trim','duration_time']])
print(ride_sharing["duration_time"].mean())




--------------- Data range constraints

# import data time:
import datetime as dt
today_date = dt. date. today()
user_signups[user_signups['date'] > dt.date.today()]


Deal with out of range data:
- dropping data
- setting custom min and max
- treat as missing and impute
- setting custom value depending on business assumptions

import pandas as pd
# output Movies with rating > 5
movies[movies['avg_rating']>5]
# drop values using filtering
movies = movies[movies['avg_rating'] <= 5 ]
# drop values via .drop()
movies.drop(movies[movies['avg_rating'] > 5].index, inplace= True)
# Assert values
assert movies['avg_rating'].max() <= 5
----  In Python, the assert statement is used to continue the execute if the given condition evaluates to True
# converting avg_rating > 5 to 5
movies.loc[movies['avg_rating'] > 5, 'avg_rating'] = 5 -->  loc. Access a group of rows and columns by label
# Assert statement
assert movies['avg_rating']max() <= 5


Date Range example:
import datetime as dt
import pands as pd
# output data types
user_signup_date.dtypes. --> .dtypes
# Convert to datetime
user_signup['user_signup_date'] = pd.to_datetime(user_signup['user_signup_date']) ---> .to_datetime()
# Assert that conversion happens
assert user_signup['user_signup_date'].dtype == 'datetime64[ns]'

today_date = dt.date.today()
# Drop the values using filtering
user_singups= user_singnups[user_singups['subscription_date'] < today_date]
# Drop values using .drop()
user_singups.drop(user_singups[user_singups['subscription_date'] > today_date, inplace=True)
# drop values using filtering
user_signups.loc[user_singups['subscription_date'] > today_date, 'subscription_date']=today_date
# Assert is true
assert user_signups.subscription_date.max().date() <= today_date


Exercise
# Convert ride_date to datetime
ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date'])

# Print maximum of ride_dt column
print(ride_sharing['ride_dt'].max())





----- Uniqueness constraints
duplicate values:
# how to find duplicate values
duplicates = height_weight.duplicated() ---> .duplicated()
height_weight[duplicates]

.duplicated()
-- subset: list of column to check for duplicates
-- keep: Whether to keep 'first', 'last' or 'False' (all) duplicated values

duplicates = height_weight.duplicated(subset = column_names, keep = False)
# output
height_weight[duplicates].sort_values(by = ' first_name')


-- inplace: drop duplicated rows directly inside DataFrame without creating new object (inplace=True)
column_names =['first_name','last_name','address']

height_weight.drop_duplicates(inplace=True)



How to treat duplicate values?
 .groupby() and .agg() method
 
 column_names =['first_name','last_name','address']
 
 # Create  summaries dictionary for aggregation function
 summaries {'height':'max','weight':'mean'}
 
 height_weight = height_weight.groupby(by = column_names). agg(summaries).rest_index()
 
 ---> we chain this entire line with the .reset_index() method. so that we can have numbered indicies in the final output
 # Make sure aggregation is done
 duplicates = height_weight.duplicated(subset = column_names,keep=False)
 height_weights[duplicates].sort_values(by='first_name')
 
 
 
 # Group by ride_id and compute new statistics
ride_unique = ride_sharing.groupby('ride_id').agg(statistics).reset_index()

# Find duplicated values again
duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)
duplicated_rides = ride_unique[duplicates == True]
 
 # Assert duplicates are processed
assert duplicated_rides.shape[0] == 0


########################################## Course 2: Text and categorical data problems##########################################

Finding inconsitent categories
inconsistent_categories = set (study_date['blood_type']). difference(categories['blood_type']) --> set(data['column'].difference(categories['column'])
# get and print rows with incosistent categories
inconsistent_rows = study_date['blood_types'].isin(inconsistent_categories) ---> df['column'].isin(xxxx)
study_data[inconsistent_rows]

Dropping inconsistent categories
# drop inconsistent categories and get consistent data only
consistent_data = study_data[~inconsistent_rows] --> ~xxx



# Print categories DataFrame
print(categories)

# Print unique values of survey columns in airlines
print('Cleanliness: ', airlines['cleanliness'].unique(), "\n")
print('Safety: ', airlines['safety'].unique(), "\n")
print('Satisfaction: ', airlines['satisfaction'].unique(), "\n")


# Print rows with inconsistent category
print(airlines[cat_clean_rows])

# Print rows with consistent categories only
print(airlines[~cat_clean_rows])




------ Categorical variables

capitalization

marriage_status = demographics-'marriage_status']
marriage_status.value_counts()
# get value counts on DataFrame
marriage_status.groupby('marriage_status').count()


Value consistency
# Capitalize
marriage_status['marriage_status'] = marriage_status['marriage_status'].str.upper()
marriage_status['marriage_status'].value_counts()
# Lower Case
marriage_status['marriage_status'] = marriage_status['marriage_status'].str.lower()
marriage_status['marriage_status'].value_counts()

## Strip all spaces
demographics = demographics['marriage_status'].str.strip()
demographics['marriage_status'].value_counts()



# Collapsing data into categories
create categories out of data : income_group column from income column

# Using qcut()
import pandas as pd
ranges[0,200000,5000000,np.inf]
group_names =['0-200k','200k-500k','500k+']
demographics['income_group'] = pd.qcut(demographics['household_income'],q=3,bins=ranges,labels=groups_names)

# print income_group column
demographics[['income_group','household_group']]





# map categories to fewer ones : reducing categories in categorical column
operating_system: 'Microsoft','MacOs','IOS','Andriod','Linux'
operating_system column should be 'DesktopOS','MobileOS'

# Creating mapping dictinary and replace
mapping = {'Microsoft':'DesktopOS','MacOs':'DesktopOS','IOS':'MobileOS','Andriod':'MobileOS','Linux':'DesktopOS'}

devices['operating_system'] = devices['operating_system'].replace(mapping)
devices['operating_system'].unique()



# Print unique values of both columns
print(airlines['dest_region'].unique())
print(airlines['dest_size'].unique())

# Lower dest_region column and then replace "eur" with "europe"
airlines['dest_region'] = airlines['dest_region'].str.lower()
airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})

# Remove white spaces from `dest_size` --- !!!!!!!!!!!!
airlines['dest_size'] = airlines['dest_size'].str.strip()






# Create ranges for categories
label_ranges = [0, 60, 180, np.inf]
label_names = ['short', 'medium', 'long']

# Create wait_type column
airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, labels = label_names)

# Create mappings and replace
mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', 
            'Thursday': 'weekday', 'Friday': 'weekday', 
            'Saturday': 'weekend', 'Sunday': 'weekend'}

airlines['day_week'] = airlines['day'].replace(mappings)




-- Cleaning Text Data
# replace + with 00
phones['number'] = phones['number'].str.replace("+","00")

# replace phone numbers with lower than 10 digits to NaN
digits = phones['number'].str.len()
phones.loc[digits < 10,"number"] = np.nan --> NaN : np.nan


# Assert minimum phone number length is 10
sanity_check = phone['number'].str.len()
assert sanity_check.min() >= 10
# Assert all numbers do not have + or ---->!!!!!!!!!!!
assert phone['number'].str.contains("+|-").any() == False
---> Remember: assert returns nothing is the condition passes

# replace letters with nothing. --->!!!!!!!!!!!
phone['number']=phone['number'].str.replace(r'\D+','')



# Store length of each row in survey_response column
resp_length = airlines['survey_response'].str.len() --> find length : str.len()

# Find rows in airlines where resp_length > 40
airlines_survey = airlines[resp_length > 40] --> remember this format df[length_variable > xx]

# Assert minimum survey_response length is > 40
assert airlines_survey['survey_response'].str.len().min() > 40

# Print new survey_response column
print(airlines_survey['survey_response'])




########################################## Course 3: Advanced data problems  ##########################################




##########################################
