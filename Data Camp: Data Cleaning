########################################## Course 1: Common data problems ##########################################
----- Data type

# get the data types of columns
sales.dtypes

# print sum of all Revenue column
sales['Revenue'].sum()

String to integers: --> str.strip('')
# Remove $ from Revenue column
sales['Revenue'] = sales['Revenue'].str.strip('$') --> .str.strip('xx')
sales['Revenue'] = sales['Revenue'].astype('int')

# verify that Revenue is now an integer
assert sales['Revenue'].dtype == 'int'


Numeric or categorical?
# convert to categorical:
df['marriage_status'] = df['marriage_status'].astype('category')

# Print the information of ride_sharing
print(ride_sharing.info()) ---> data.info()

# Print summary statistics of user_type column
print(ride_sharing['user_type'].describe()) --> data.decribe()

# Convert user_type from integer to category
ride_sharing['user_type_cat'] = ride_sharing['user_type'].astype('category)

# Write an assert statement confirming the change
assert ride_sharing['user_type_cat'].dtype == 'category'

# Print new summary statistics 
print(ride_sharing['user_type_cat'].describe())

# Strip duration of minutes
ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip("minutes")

# Print formed columns and calculate average ride duration 
print(ride_sharing[['duration','duration_trim','duration_time']])
print(ride_sharing["duration_time"].mean())




--------------- Data range constraints

# import data time:
import datetime as dt
today_date = dt. date. today()
user_signups[user_signups['date'] > dt.date.today()]


Deal with out of range data:
- dropping data
- setting custom min and max
- treat as missing and impute
- setting custom value depending on business assumptions

import pandas as pd
# output Movies with rating > 5
movies[movies['avg_rating']>5]
# drop values using filtering
movies = movies[movies['avg_rating'] <= 5 ]
# drop values via .drop()
movies.drop(movies[movies['avg_rating'] > 5].index, inplace= True)
# Assert values
assert movies['avg_rating'].max() <= 5
----  In Python, the assert statement is used to continue the execute if the given condition evaluates to True
# converting avg_rating > 5 to 5
movies.loc[movies['avg_rating'] > 5, 'avg_rating'] = 5 -->  loc. Access a group of rows and columns by label
# Assert statement
assert movies['avg_rating']max() <= 5


Date Range example:
import datetime as dt
import pands as pd
# output data types
user_signup_date.dtypes. --> .dtypes
# Convert to datetime
user_signup['user_signup_date'] = pd.to_datetime(user_signup['user_signup_date']) ---> .to_datetime()
# Assert that conversion happens
assert user_signup['user_signup_date'].dtype == 'datetime64[ns]'

today_date = dt.date.today()
# Drop the values using filtering
user_singups= user_singnups[user_singups['subscription_date'] < today_date]
# Drop values using .drop()
user_singups.drop(user_singups[user_singups['subscription_date'] > today_date, inplace=True)
# drop values using filtering
user_signups.loc[user_singups['subscription_date'] > today_date, 'subscription_date']=today_date
# Assert is true
assert user_signups.subscription_date.max().date() <= today_date


Exercise
# Convert ride_date to datetime
ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date'])

# Print maximum of ride_dt column
print(ride_sharing['ride_dt'].max())





----- Uniqueness constraints
duplicate values:
# how to find duplicate values
duplicates = height_weight.duplicated() ---> .duplicated()
height_weight[duplicates]

.duplicated()
-- subset: list of column to check for duplicates
-- keep: Whether to keep 'first', 'last' or 'False' (all) duplicated values

duplicates = height_weight.duplicated(subset = column_names, keep = False)
# output
height_weight[duplicates].sort_values(by = ' first_name')


-- inplace: drop duplicated rows directly inside DataFrame without creating new object (inplace=True)
column_names =['first_name','last_name','address']

height_weight.drop_duplicates(inplace=True)



How to treat duplicate values?
 .groupby() and .agg() method
 
 column_names =['first_name','last_name','address']
 
 # Create  summaries dictionary for aggregation function
 summaries {'height':'max','weight':'mean'}
 
 height_weight = height_weight.groupby(by = column_names). agg(summaries).rest_index()
 
 ---> we chain this entire line with the .reset_index() method. so that we can have numbered indicies in the final output
 # Make sure aggregation is done
 duplicates = height_weight.duplicated(subset = column_names,keep=False)
 height_weights[duplicates].sort_values(by='first_name')
 
 
 
 # Group by ride_id and compute new statistics
ride_unique = ride_sharing.groupby('ride_id').agg(statistics).reset_index()

# Find duplicated values again
duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)
duplicated_rides = ride_unique[duplicates == True]
 
 # Assert duplicates are processed
assert duplicated_rides.shape[0] == 0


########################################## Course 2: Text and categorical data problems##########################################

Finding inconsitent categories
inconsistent_categories = set (study_date['blood_type']). difference(categories['blood_type']) --> set(data['column'].difference(categories['column'])
# get and print rows with incosistent categories
inconsistent_rows = study_date['blood_types'].isin(inconsistent_categories) ---> df['column'].isin(xxxx)
study_data[inconsistent_rows]

Dropping inconsistent categories
# drop inconsistent categories and get consistent data only
consistent_data = study_data[~inconsistent_rows] --> ~xxx



# Print categories DataFrame
print(categories)

# Print unique values of survey columns in airlines
print('Cleanliness: ', airlines['cleanliness'].unique(), "\n")
print('Safety: ', airlines['safety'].unique(), "\n")
print('Satisfaction: ', airlines['satisfaction'].unique(), "\n")


# Print rows with inconsistent category
print(airlines[cat_clean_rows])

# Print rows with consistent categories only
print(airlines[~cat_clean_rows])




------ Categorical variables

capitalization

marriage_status = demographics-'marriage_status']
marriage_status.value_counts()
# get value counts on DataFrame
marriage_status.groupby('marriage_status').count()


Value consistency
# Capitalize
marriage_status['marriage_status'] = marriage_status['marriage_status'].str.upper()
marriage_status['marriage_status'].value_counts()
# Lower Case
marriage_status['marriage_status'] = marriage_status['marriage_status'].str.lower()
marriage_status['marriage_status'].value_counts()

## Strip all spaces
demographics = demographics['marriage_status'].str.strip()
demographics['marriage_status'].value_counts()



# Collapsing data into categories
create categories out of data : income_group column from income column

# Using qcut()
import pandas as pd
ranges[0,200000,5000000,np.inf]
group_names =['0-200k','200k-500k','500k+']
demographics['income_group'] = pd.qcut(demographics['household_income'],q=3,bins=ranges,labels=groups_names)

# print income_group column
demographics[['income_group','household_group']]





# map categories to fewer ones : reducing categories in categorical column
operating_system: 'Microsoft','MacOs','IOS','Andriod','Linux'
operating_system column should be 'DesktopOS','MobileOS'

# Creating mapping dictinary and replace
mapping = {'Microsoft':'DesktopOS','MacOs':'DesktopOS','IOS':'MobileOS','Andriod':'MobileOS','Linux':'DesktopOS'}

devices['operating_system'] = devices['operating_system'].replace(mapping)
devices['operating_system'].unique()



# Print unique values of both columns
print(airlines['dest_region'].unique())
print(airlines['dest_size'].unique())

# Lower dest_region column and then replace "eur" with "europe"
airlines['dest_region'] = airlines['dest_region'].str.lower()
airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})

# Remove white spaces from `dest_size` --- !!!!!!!!!!!!
airlines['dest_size'] = airlines['dest_size'].str.strip()






# Create ranges for categories
label_ranges = [0, 60, 180, np.inf]
label_names = ['short', 'medium', 'long']

# Create wait_type column
airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, labels = label_names)

# Create mappings and replace
mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', 
            'Thursday': 'weekday', 'Friday': 'weekday', 
            'Saturday': 'weekend', 'Sunday': 'weekend'}

airlines['day_week'] = airlines['day'].replace(mappings)




-- Cleaning Text Data
# replace + with 00
phones['number'] = phones['number'].str.replace("+","00")

# replace phone numbers with lower than 10 digits to NaN
digits = phones['number'].str.len()
phones.loc[digits < 10,"number"] = np.nan --> NaN : np.nan


# Assert minimum phone number length is 10
sanity_check = phone['number'].str.len()
assert sanity_check.min() >= 10
# Assert all numbers do not have + or ---->!!!!!!!!!!!
assert phone['number'].str.contains("+|-").any() == False
---> Remember: assert returns nothing is the condition passes

# replace letters with nothing. --->!!!!!!!!!!!
phone['number']=phone['number'].str.replace(r'\D+','')



# Store length of each row in survey_response column
resp_length = airlines['survey_response'].str.len() --> find length : str.len()

# Find rows in airlines where resp_length > 40
airlines_survey = airlines[resp_length > 40] --> remember this format df[length_variable > xx]

# Assert minimum survey_response length is > 40
assert airlines_survey['survey_response'].str.len().min() > 40

# Print new survey_response column
print(airlines_survey['survey_response'])




########################################## Course 3: Advanced data problems  ##########################################
Uniformity

temperature 32C = 89.2F
weight 20kg = 25pounds

Treating temperature data: C = (F-32) * 5/9

temp_fah = temperatures.loc['temperatures['temperature'] > 40, 'temperature']
temp_cels = (temp_fah - 32) * 5/9
temperatures.loc['temperatures['temperature'] > 40, 'temperature'] == temp_cels
# Assert conversion is correct
assert temperatures['temperature'].max() < 40


Treating date data
pd.to_datetime() --> recognize formats automatically

birthdays['Birthday']=pd.to_datetime(birthdays['Birthday'], infer_datatime_format=True,errors='coerce')

birthday['Birthday'] = birthday['Birthday'].dt.strftime("%d-%m-%Y")



Treating ambiguous date data
For example, 2019-03-08 is for Aguest or for March?
-- a. Set them to NA and drop them
   b. Infer the format of the data in question by checking the format of subsequent and previous values.
   c. Infer the format from the original data source.
   
   
   
# Find values of acct_cur that are equal to 'euro'
acct_eu = banking['acct_cur'] == 'euro'

# Convert acct_amount where it is in euro to dollars
# Find all the rows of acct_amount in banking that fit the acct_eu condition, and convert them to USD by multiplying them with 1.1
banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1 

# Unify acct_cur column by changing 'euro' values to 'dollar'
banking.loc[acct_eu, 'acct_cur'] = 'dollar'

# Assert that only dollar currency remains
assert banking['acct_cur'].unique() == 'dollar'




# Print the header of account_opend
print(banking['account_opened'].head())

# Convert account_opened to datetime
banking['account_opened'] = pd.to_datetime(banking['account_opened'],
                                           # Infer datetime format
                                           infer_datetime_format = True,
                                           # Return missing value for error
                                           errors = 'coerce')  

# Get year of account opened
banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')

# Print acct_year
print(banking['acct_year'])




------ Cross field validation
The use of multiple fields in a dataset to sanity check data integrity

sum_classes = fligts[ ['eco','bus','first']].sum(axis=1) --> axis1 indicates row wise summing
passer_equ = sum_classes == flights['total_passengers']

# Find and filter out rows with inconsistent passenger totals
inconsistent_pass = flight[~passer_equ]
consistent_pass = flight[passer_equ]



another example:
import pandas as pd
import datetime as dt

# convert datetime and get today's date
users["birthday'] = pd.to_datetime(users['birthday'])
today = dt.date.today()
# for each row in the birthday column, calculate the year difference
age_manual = today.year - users['birthday'].dt.year
# Find instances where ages match
age_equ = age_manual == users['Age']
# find 
inconsistent_age = users[~age_equ]
consistent_age=users[age_equ]


--- Exercise

# Store fund columns to sum against
fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']

# Find rows where fund_columns row sum == inv_amount
inv_equ = banking[fund_columns].sum(axis = 1) == banking['inv_amount']

# Store consistent and inconsistent data
consistent_inv = banking[inv_equ]
inconsistent_inv = banking[~inv_equ]

# Store consistent and inconsistent data
print("Number of inconsistent investments: ", inconsistent_inv.shape[0])





# Store today's date and find ages
today = dt.date.today()
ages_manual = today.year - banking['birth_date'].dt.year

# Find rows where age column == ages_manual
age_equ = ages_manual == banking['age']

# Store consistent and inconsistent data
consistent_ages = banking[age_equ]
inconsistent_ages = banking[~age_equ]

# Store consistent and inconsistent data
print("Number of inconsistent ages: ", inconsistent_ages.shape[0])





---- Completeness

missing data: NA,nan,0,.

# returing missing values
airquality.isna()
# get summary of missingness
airquality.isna().sum()

# visulisation
import missingno as msno
import matplotlib.pyplot as plt
msno.matrix(airquality)
plt.show()




# isolate missing and complete values aside
missing = airquality[airquality['CO2'].isna()]
complete = airquality[~airquality['CO2}.isna()]
# describe complete DF
complete.describe()
missing.describe()

sorted_airquality = airquality.sort_values(by = ' temperature')
msno.matrix(sorted_airquality)
plt.show()


[ How to deal with missing values ]
simple approaches:
a. drop missing values
b. impute with statistical measures (mean, median, mode,...)
more complex approaches:
a. impute using an algorithm approach
b. impute with machine learning models


- dropping missing values
airquality_dropped = airquality.dropna(subset=['CO2'] --> .dropna()

- replacing with statistical measures
co2_mean = airquality['CO2'].mean()
airquality_imputed = airquality.fillna({'CO2':co2_mean}) ---> .fillna() where .fillna({'xx':'replaceBy'})



-- exercise
# Drop missing values of cust_id
banking_fullid = banking.dropna(subset = ['cust_id']) --> subset =' '

# Compute estimated acct_amount
acct_imp = banking_fullid['inv_amount'] * 5 

# Impute missing acct_amount with corresponding acct_imp
banking_imputed = banking_fullid.fillna({'acct_amount':acct_imp})

# Print number of missing values
print(banking_imputed.isna().sum())


########################################## Course 4: Record linkage  ##########################################
Comparing strings











