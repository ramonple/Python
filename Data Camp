# Put the x-axis on a logarithmic scale
plt.xscale('log')


# Show and clean up again
plt.show()
plt.clf()


# change the opacity of bubble (scatter plot)
plt.scatter(x = gdp_cap, y = life_exp, s = np.array(pop) * 2,c=col,alpha=0.8) # s:size,c:color


# add grid lines
after plt.text() -> add particular text for particular point
plt.grid(True)


# Get index of 'germany': ind_ger
ind_ger = countries.index('germany')
# Use ind_ger to print out capital of Germany
print(capitals[ind_ger])


# Print out the keys in europe
print(europe.keys())
# Print out value that belongs to key 'norway'
print(europe['norway'])


           List                  Vs                Dictionary
Select, update and remove: []            Select, update and remove: []
indexed by range of numbers              indexed by unique keys
Collection of values, order matters      Lookup with unique keys


# add things to dictionary
dic_name['new_key'] = corresponding_value

## cars['cars_per_cap']
cars[['cars_per_cap']]
The single bracket version gives a Pandas Series, the double bracket version gives a Pandas DataFrame.

# set the first column as the index
df = pd.read_csv('data.csv', index_col=0)

## loc and iloc
With loc and iloc you can do practically any data selection operation on DataFrames you can think of. 
loc is label-based, which means that you have to specify rows and columns based on their row and column labels. 
iloc is integer index based, so you have to specify rows and columns by their integer index like you did in the previous exercise.


## Not And Or
Notice that not has a higher priority than and and or, it is executed first.


## find the subset 
You can use   cars[cars['drives_right']] to build sel

## enumerate()
for index, area in enumerate(areas) :
    print("room" + str(index) + ": " + str(area))


######## Numpy: np.nditer
loop over 2D Numpy array
for x in np.nditer(my_array):

## Pandas: iterrows
Iterating over a Pandas DataFrame is typically done with the iterrows()
for x,y in name.iterrows():

Write a for loop that iterates over the rows of cars and on each iteration perform two print() calls: 
one to print out the row label and one to print out all of the rows contents. !!!!!!
( how to show the row name and corresponding row contents?)
for lab, row in cars.iterrows() :
    print(lab)
    print(row)

The row data that's generated by iterrows() on every run is a Pandas Series. This format is not very convenient to print out.
Luckily, you can easily select variables from the Pandas Series using square brackets:

for lab, row in brics.iterrows() :
    print(row['country'])
    
## upper the corresponding colum:
row["country"].upper()


Using iterrows() to iterate over every observation of a Pandas DataFrame is easy to understand, 
but not very efficient. On every iteration, you're creating a new Pandas Series.

If you want to add a column to a DataFrame by calling a function on another column, 
the iterrows() method in combination with a for loop is not the preferred way to go. Instead, you'll want to use apply().

##########################################################  
for lab, row in brics.iterrows() :
    brics.loc[lab, "name_length"] = len(row["country"])

brics["name_length"] = brics["country"].apply(len)
############################################################ 

## Pandas: apply
cars["COUNTRY"] = cars["country"].apply(str.upper)


# random generator

np.random.seed(123)
np.random.rand()
 -- consistent between runs
 
 np.random.randint(0,2) # randomly generate 0 or 1
 np.random.randint(a,b) # randomly generater interger between a and b-1
 


# Initialize random_walk
random_walk=[0] # Make a list random_walk that contains the first step, which is the integer 0.

for x in range(100) : # The loop should run 100 times.
    # Set step: last element in random_walk
    step = random_walk[-1] # On each iteration, set step equal to the last element in the random_walk list. You can use the index -1 for this.

    # Roll the dice
    dice = np.random.randint(1,7)

    # Determine next step
    if dice <= 2:
        step = max(step - 1)
    elif dice <= 5:
        step = step + 1
    else:
        step = step + np.random.randint(1,7) # re-run the dice

    # append next_step to random_walk
    random_walk.append(step)

# Print random_walk
print(random_walk)



##  Simulate multiple walks

# initialize and populate all_walks
all_walks = []
for i in range(10) :
    random_walk = [0]
    for x in range(100) :
        step = random_walk[-1]
        dice = np.random.randint(1,7)
        if dice <= 2:
            step = max(0, step - 1)
        elif dice <= 5:
            step = step + 1
        else:
            step = step + np.random.randint(1,7)
        random_walk.append(step)
    all_walks.append(random_walk)
    




################################# Data Manipulation with pandas ############################################################################################################################################################################
https://yuleii.github.io/2020/06/27/data-manipulation-with-pandas.html
############################################################################################################################################################################
COURSE 1: DataFrames

.values: A two-dimensional NumPy array of values.
.columns: An index of columns: the column names.
.index: An index for the rows: either row numbers or row names.

Sorting rows
homelessness_ind = homelessness.sort_values('individuals')
# Sort homelessness by descending family members
homelessness_fam = homelessness.sort_values('family_members',ascending=False)
# Sort homelessness by region, then descending family members
homelessness_reg_fam = homelessness.sort_values(['region','family_members'], ascending = [True, False]

Subsetting columns
# Select the individuals column
individuals = homelessness['individuals']
# Select the state and family_members columns
state_fam = homelessness[['state','family_members']] # double brackets
# Select only the individuals and state columns, in that order
ind_state = homelessness[['individuals','state']]

Subsetting rows
# Filter for rows where individuals is greater than 10000
ind_gt_10k = homelessness[homelessness['individuals']>10000]
# Filter for rows where region is Mountain
mountain_reg = homelessness[homelessness['region']=="Mountain"]
# Filter for rows where family_members is less than 1000 
# and region is Pacific
fam_lt_1k_pac = homelessness[(homelessness['family_members']<1000) & (homelessness['region']=="Pacific")]


Subsetting rows by categorical variables
|, .isin()
# Subset for rows in South Atlantic or Mid-Atlantic regions
south_mid_atlantic = homelessness[(homelessness['region']=="South Atlantic") | (homelessness['region']=="Mid-Atlantic")] #[()&()]
# The Mojave Desert states
canu = ["California", "Arizona", "Nevada", "Utah"]
# Filter for rows in the Mojave Desert states
mojave_homelessness = homelessness[homelessness['state'].isin(canu)]


New columns
# Add total col as sum of individuals and family_members
homelessness['total'] = homelessness['individuals'] + homelessness['family_members']
# Add p_individuals col as proportion of individuals
homelessness['p_individuals'] = homelessness['individuals'] / homelessness['total']
# See the result
print(homelessness)
Combo-attack!
# Create indiv_per_10k col as homeless individuals per 10k state pop
homelessness["indiv_per_10k"] = 10000 * homelessness["individuals"] / homelessness["state_pop"]
# Subset rows for indiv_per_10k greater than 20
high_homelessness = homelessness[homelessness['indiv_per_10k']>20]
# Sort high_homelessness by descending indiv_per_10k
high_homelessness_srt = high_homelessness.sort_values('indiv_per_10k', ascending=False)
# From high_homelessness_srt, select the state and indiv_per_10k cols
result = high_homelessness_srt[['state','indiv_per_10k']]
# See the result
print(result)



COURSE 2: Aggregating Data


Counting

Dropping duplicates
# Drop duplicate store/type combinations
store_types = sales.drop_duplicates(subset=['store','type'])
print(store_types.head())

# Drop duplicate store/department combinations
store_depts = sales.drop_duplicates(subset=['store','department'])
print(store_depts.head())

# Subset the rows that are holiday weeks and drop duplicate dates
holiday_dates = sales[sales['is_holiday']==True].drop_duplicates('date')

# Print date col of holiday_dates
print(holiday_dates['date'])


Counting categorical variables
# Count the number of stores of each type
store_counts = store_types['type'].value_counts()
print(store_counts)

# Get the proportion of stores of each type
store_props = store_types['type'].value_counts(normalize=True) 
# By setting normalize=True , the object returned will contain the relative frequencies of the unique values
print(store_props)

# Count the number of each department number and sort
dept_counts_sorted = store_depts['department'].value_counts(sort=True)
# sort: bool, default True Sort by frequencies.
print(dept_counts_sorted)

# Get the proportion of departments of each number and sort
dept_props_sorted = store_depts['department'].value_counts(sort=True, normalize=True)
print(dept_props_sorted)











####################################################################################################################################


#################################  Data Cleaning  ##################################################################

# Strip duration of minutes
ride_sharing['duration_trim'] = ride_sharing['duration'].str.strip("minutes")

# Convert duration to integer
ride_sharing['duration_time'] = ride_sharing["duration_trim"].astype(int)

# Write an assert statement making sure of conversion
assert ride_sharing['duration_time'].dtype == 'int'

# Print formed columns and calculate average ride duration
print(ride_sharing[['duration','duration_trim','duration_time']])
print((ride_sharing['duration_time']).mean())



# table value constraints

Use .loc[] to set all values of xx in a range

# Convert tire_sizes to integer ->xxx.astype(' ')
ride_sharing['tire_sizes'] = ride_sharing['tire_sizes'].astype('int')

example" Set all values above 27 to 27 -- .loc[xxx>xx,xxx] = yyy
ride_sharing.loc[ride_sharing["tire_sizes" >27, "tire_sizes"] = 27


# Convert ride_date to datetime
ride_sharing['ride_dt'] = pd.to_datetime(ride_sharing['ride_date'])

# Save today's date
today = dt.date.today()

# Set all in the future to today's date
ride_sharing.loc[ride_sharing['ride_dt'] > today, 'ride_dt'] = today


### uniqueness constraints -> duplicated values
- find the duplicated values: .duplicated()
.duplicated(subset = , keep=)
subset: list of column names to check for duplication
keep: whether to keep first('first'), last('last') or all(False) duplicated values


duplicates = df.duplicated()
print(duplicates) -> will print true or false
df[duplicates] -> output the duplicated information
dr[duplicates].sort_values(by ='xxx')

How to treat duplicate values?
.drop_duplicates()
inplace: drop duplicated rows directly inside DateFrame without creating new objects, inplace = True

.groupby()
.agg()



### Membership constraints

> finding inconsistent categories
inconsistent_categories = set(study_data['blood_type']).difference(categories['blood_type'])
print(inconsistent_categories)

> find the incosistent rows
inconsistent_rows = study_date['blood_type'].isin(inconsistent_catogies)

> drop inconsistent rows
inconsistent_data = study_data[incosistent_rows] - find
# Drop
inconsistent_data = study_data[~incosistent_rows]

## Categorical variables
a. value consistency
Upper and Lower case:
1. marriage_status['marriage_status'] = marriage_status['marriage_status'].str.upper()
   marriage_status['marriage_status'] = marriage_status['marriage_status'].str.lower()
b. blank space
   marriage_status['marriage_status'] = marriage_status['marriage_status'].str.strip()
c. collapsing data into categories
  1) >>> create categories out of data: income_group column from income column
    import pandas as pd
    ranges=[0,200000,500000,np.inf]
    group_names = ['0-200k','200k-500k','500k+']
    demographics['income_group']=pd.cut(demographics['household_income'],bins=ranges,labels = group_names)
    demographics[['income_group'],['household_income']]
 
 example:
 # Create ranges for categories
label_ranges = [0, 60, 180, np.inf]
label_names = ['short', 'medium', 'long']

# Create wait_type column
airlines['wait_type'] = pd.cut(airlines['wait_min'], bins = label_ranges, labels = label_names)

# Create mappings and replace
mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', 
            'Thursday': 'weekday', 'Friday': 'weekday', 
            'Saturday': 'weekend', 'Sunday': 'weekend'}

airlines['day_week'] = airlines['day'].replace(mappings)

  
  2) >>> map categories to fewer ones
  operation_system column is 'a','b','c','d','e'
  operation_system column should be ' abc','de'
  
  mapping ={a':'abc','b':'abc,'c':'abc,'d':'de','e':'de'}
  devices['operation_system']=devices['operating_system'].replace(mapping)
  devices['operation_system'].unique()
  
  d. replace
  airlines['dest_region'] = airlines['dest_region'].replace({'eur':'europe'})
    



>>>>>>>>>>>  Cleaning text data
# replace '+" with '00'
phones["phone number"] = phones["phones number"].str.replace("+","00")
# replace phone number with lower than 10 digits to Nan
digits = phones["phone number"] .str.len()
phones.loc[digicts < 10, "phone number"] = np.nan

-- example:
# Store length of each row in survey_response column
resp_length = airlines['survey_response'].str.len()

# Find rows in airlines where resp_length > 40
airlines_survey = airlines[resp_length > 40]

# Assert minimum survey_response length is > 40
assert airlines_survey['survey_response'].str.len().min() > 40

# Print new survey_response column
print(airlines_survey['survey_response'])


# regular expresion
# replace letters with nothing
phones["phone number"] = phones["phones number"].str.replae(r'\D+','')
--  r'(\d+)' This is a Regular Expression pattern \d is a regex pattern for digit
-- + is a regex pattern for at least (one or more) since they are enclosed in a ( ) that means the group that you want to capture



## Uniformity
pandas.to_datetime()
birthday["Birthay'] = birthday['Birthday'].dt.strftime('%d-%m-%Y')

# Find values of acct_cur that are equal to 'euro'
acct_eu = banking['acct_cur'] == 'euro'

# Convert acct_amount where it is in euro to dollars
banking.loc[acct_eu, 'acct_amount'] = banking.loc[acct_eu, 'acct_amount'] * 1.1 

# Unify acct_cur column by changing 'euro' values to 'dollar'
banking.loc[acct_eu, 'acct_cur'] = 'dollar'

# Assert that only dollar currency remains
assert banking['acct_cur'].unique() == 'dollar'



# Print the header of account_opend
print(banking['account_opened'].head())

# Convert account_opened to datetime
banking['account_opened'] = pd.to_datetime(banking['account_opened'],
                                           # Infer datetime format
                                           infer_datetime_format = True,
                                           # Return missing value for error
                                           errors = 'coerce')  

# Get year of account opened
banking['acct_year'] = banking['account_opened'].dt.strftime('%Y')

# Print acct_year
print(banking['acct_year'])




 >>>>>>>>>>>> Grouped summary statistics
.groupby()

What percent of sales occurred at each store type?
# Calc total weekly sales
sales_all = sales["weekly_sales"].sum()
# Subset for type A stores, calc total weekly sales
sales_A = sales[sales["type"] == "A"]["weekly_sales"].sum()
# Subset for type B stores, calc total weekly sales
sales_B = sales[sales["type"] == "B"]["weekly_sales"].sum()
# Subset for type C stores, calc total weekly sales
sales_C = sales[sales["type"] == "C"]["weekly_sales"].sum()
# Get proportion for each type
sales_propn_by_type = [sales_A, sales_B, sales_C] / (sales_A+sales_B+sales_C)
print(sales_propn_by_type)

Calculations with .groupby()
# Get proportion for each type
sales_propn_by_type = sales_by_type/sum(sales.weekly_sales) 
# Group by type and is_holiday; calc total weekly sales
sales_by_type_is_holiday = sales.groupby(["type",'is_holiday'])["weekly_sales"].sum()

# For each store type, aggregate weekly_sales: get min, max, mean, and median
sales_stats = sales.groupby('type')['weekly_sales'].agg([np.min, np.max, np.mean, np.median])
# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median
unemp_fuel_stats = sales.groupby('type')[['unemployment', 'fuel_price_usd_per_l']].agg([np.min, np.max, np.mean, np.median])


Pivot table
---- The .pivot_table() method is just an alternative to .groupby().
# Pivot for mean weekly_sales for each store type
mean_sales_by_type = sales.pivot_table(values='weekly_sales', index='type')
# Pivot for mean and median weekly_sales for each store type
mean_med_sales_by_type = sales.pivot_table(values='weekly_sales', index= 'type', aggfunc=[np.mean, np.median])
# Pivot for mean weekly_sales by store type and holiday 
mean_sales_by_type_holiday = sales.pivot_table(values='weekly_sales', index= 'type', columns='is_holiday')


Fill in missing values and sum values with pivot tables
The .pivot_table() method has several useful arguments, including fill_value and margins.

fill_value replaces missing values with a real value (known as imputation).
margins is a shortcut for when you pivoted by two variables, 
but also wanted to pivot by each of those variables separately: it gives the row and column totals of the pivot table contents.

# Print mean weekly_sales by department and type; fill missing values with 0
print(sales.pivot_table(values='weekly_sales', index='department', columns='type', fill_value=0))
# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols
print(sales.pivot_table(values="weekly_sales", index="department", columns="type", fill_value=0, margins=True))




 >>>>>>>>>>>> Slicing and indexing

.set_index(), reset_index()

---- Explicit index
Setting & removing indexes




